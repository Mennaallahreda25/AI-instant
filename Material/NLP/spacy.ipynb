{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "848e60fa",
   "metadata": {},
   "source": [
    " # SpaCy Overview\n",
    " \n",
    "What is spaCy(v2):\n",
    "spaCy is an open-source software library for advanced Natural Language Processing, written in the programming languages Python and Cython.\n",
    "\n",
    "The library is published under the MIT license and currently offers statistical neural network models for English, German, Spanish, Portuguese, French, Italian, Dutch and multi-language NER, as well as tokenization for various other languages.\n",
    "spaCy v2.0 features new neural models\n",
    "\n",
    "for tagging, parsing and entity recognition. The models have been designed and implemented from scratch specifically for spaCy, to give you an unmatched balance of speed, size and accuracy..\n",
    "\n",
    "\n",
    "\n",
    "Features:\n",
    "\n",
    "• Non-destructive tokenization          \n",
    "• Named entity recognition           \n",
    "• Support for 49+ languages             \n",
    "• 16 statistical models for 9 languages             \n",
    "• Pre-trained word vectors             \n",
    "• Easy deep learning integration          \n",
    "• Part-of-speech tagging         \n",
    "• Labelled dependency parsing         \n",
    "• Syntax-driven sentence segmentation           \n",
    "• Built in visualizers for syntax and NER             \n",
    "• Convenient string-to-hash mapping                \n",
    "• Export to numpy data arrays                \n",
    "• Efficient binary serialization              \n",
    "• Easy model packaging and deployment              \n",
    "• State-of-the-art speed           \n",
    "• Robust, rigorously evaluated accuraçy             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f93fe2b",
   "metadata": {},
   "source": [
    "# Install en & en_core_web_md en\n",
    "\n",
    "python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d65b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7090d97",
   "metadata": {},
   "source": [
    "python -m spacy link en_core_web_md en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab772304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm. load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227b9f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy. load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edb780b",
   "metadata": {},
   "source": [
    "# Tokeniz words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd152454",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://nlpforhackers.io/complete-guide-to-spacy/ 2import en_core_web_sm\n",
    "nlp = en_core'web_sm. load()\n",
    "doc = nlp( 'Hello   World!')\n",
    "for token in doc:\n",
    "    print('\"' + token.text + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5693bd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core-_web_sm\") # load the spaCy language model.\n",
    "doc = n1p ('Hello    World!')\n",
    "for token in doc:\n",
    "    print('\"' + token.text + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b749186a",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "\"Hello \"    \n",
    "\"World\"    \n",
    "\" !\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbe93cc",
   "metadata": {},
   "source": [
    "# Tokeniz sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534d22fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://nlpforhackers.io/complete-guide-to-spacy/\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "doc = nlp(\"These are apples. These are oranges.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a474f2bd",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "These are apples.             \n",
    "These are oranges.             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7cde87",
   "metadata": {},
   "source": [
    "# Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7384667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/\n",
    "# Word tokenization\n",
    "from spacy. lang.en import English\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = English()\n",
    "text = \"\"\"When learning data science, you shouldn't get discouraged!\n",
    "Challenges and setbacks aren't failures, they're just part of the journey.\n",
    "You've got this!\"\"\"\n",
    "# \"nlp\" Object is used to create documents with Linguistic annotations.\n",
    "my_doc = nlp(text)\n",
    "# Create List of word tokens\n",
    "token_list = []\n",
    "for token in my_ doc:\n",
    "    token_list.append(token. text)\n",
    "print (token_list)  # will split words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0a71ce",
   "metadata": {},
   "source": [
    "# Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6900e6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.dataquest.io/blog/tutorial-text-classification-in-\n",
    "#Stop words\n",
    "#importing stop words from English Language.\n",
    "import spacy\n",
    "spacy_stopwords = spacy. lang.en.stop_words.STOP_WORDS\n",
    "#Printing the total number of stop words:\n",
    "print('Number of stop words: %d' % len (spacy _stopwords))\n",
    "#Printing first ten stop words:\n",
    "print('First ten stop words: %' % list (spacy_stopwords) [:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b70095",
   "metadata": {},
   "source": [
    "# Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174ed5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.dataquest.io/blog/tutorial-text-classification-in-\n",
    "from spacy. lang.en import English\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = English ()\n",
    "text = \"\"\"When learning data science, you shouldn't get discour Challenges and setbacks aren't failures, \n",
    "                they're just part of\"\"\"\n",
    "\n",
    "#Implementation of stop words:\n",
    "filtered sent=[]\n",
    "# \"nlp\" Object is used to create documents with linguistic features\n",
    "doc = nlp(text)\n",
    "# filtering stop words\n",
    "for word in doc:\n",
    "    if word.is stop==False:\n",
    "        filtered_sent.append(word)\n",
    "print(\"Filtered Sentence:\"‚filtered_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9454689f",
   "metadata": {},
   "source": [
    "output:\n",
    "    \n",
    "Filtered Sentence: [learning, data, science, , discouraged, !,\n",
    "Challenges, setbacks, failures, ,, journey,\n",
    "., got, !]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34058e22",
   "metadata": {},
   "source": [
    "# Part of Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de255037",
   "metadata": {},
   "source": [
    "nine parts of speech are:\n",
    "1. noun            \n",
    "2. verb          \n",
    "3. adjective           \n",
    "4. adverb           \n",
    "5. pronoun           \n",
    "6. preposition            \n",
    "7. conjunction           \n",
    "8. interjection           \n",
    "9. article or (more recently) determiner         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e948721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy. load ('en_core_web_sm')\n",
    "doc = nlp (u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "for token in doc:\n",
    "    print(token. text, token. lemma_, token.pos_, token.tag_, token. dep_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93d59e6",
   "metadata": {},
   "source": [
    "# Part Of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631eee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy. load (\"en_core_web_sm\")\n",
    "doc = nlp(\"Next week I'll be in Madrid.\")\n",
    "print([(token. text, token.tag_) for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65f8256",
   "metadata": {},
   "source": [
    "Output: \n",
    "    \n",
    "[ ('Next', 'JJ'), ('week', 'NN'), ('I', 'PRP'),\n",
    "(\"'11\"\n",
    "'MD'), ('be', 'VB'), ('in',\n",
    "'IN'), ('Madrid'.\n",
    "\"NNP\"),\n",
    "(' .','.')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76d1919",
   "metadata": {},
   "source": [
    "# POS\n",
    "\n",
    "A word's part of speech defines its function within a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea585b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy. load(\"en_core_web_sm\")\n",
    "docs = nlp (u\"All is well that ends well.\n",
    "for word in docs:\n",
    "    print(word.text,word.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d01e2b",
   "metadata": {},
   "source": [
    "# Detecting Nouns\n",
    "\n",
    "spaCy automatically detects noun-phrases as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c3da65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy. load (\"en_core_web_sm\")\n",
    "doc = np (u\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "for chunk in doc.noun chunks:\n",
    "    print (chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb90903",
   "metadata": {},
   "source": [
    "# lemmatization\n",
    "\n",
    "Lemmatization converts words in the second or third forms to their first form variants\n",
    "It might be surprising to you but spay doesn't contain any function for stemming as it relies on lemmatization only.\n",
    "\n",
    "\n",
    "We can find the roots of all the words using spaCy lemmatization as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5616c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy. load(\"en_core_web_sm\")\n",
    "doc = n1p (u\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "for word in doc:\n",
    "    print (word.text + '===>', word. lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e10aece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy. load(\"en_core_web_sm\")\n",
    "# Implementing Lemmatization\n",
    "lem = nlp (\"run runs running runner\")\n",
    "# finding Lemma for each word\n",
    "for word in lem:\n",
    "    print (word.text,word. lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681097ae",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)\n",
    "\n",
    "What is Named Entity Recognition (NER)?    \n",
    "Named entity recognition (NER) is a sub-task of information extraction (IE) that seeks out and categories specified entities in a body or bodies of texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d47bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy. load (\"en_core_web_sm\")\n",
    "doc = nlp(\"Next week I'll be in Madrid.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a53ca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output:\n",
    "    \n",
    "Next week DATE    \n",
    "Madrid GPE\n",
    "\n",
    "\n",
    "The dataset consists of the following tags:              \n",
    "•geo = Geographical Entity              \n",
    "•org = Organization              \n",
    "•per = Person                      \n",
    "•gpe = Geopolitical Entity                  \n",
    "•tim = Time indicator                      \n",
    "•art = Artifact                               \n",
    "•eve = Event                \n",
    "•nat = Natural Phenomenon  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc4cfa5",
   "metadata": {},
   "source": [
    "# Entity Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cda66ab",
   "metadata": {},
   "source": [
    "Entity detection, also called entity recognition, is a more advanced form of language processing that identifies important elements like places, people, organizations, and languages within an input string of text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672ccca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy. load(\"en_core_web_sm\")\n",
    "nytimess nlp(u\"\"\"New York city on Tuesday declared a public health emergency and ordered mandatory measles vaccinations amid an\n",
    "At least 285 people have contracted measles in the city since September, mostly in Brooklyn's Williamsburg neighborhood. The ord\n",
    "The mandate orders all unvaccinated people in the area, including a concentration of Orthodox Jews, to receive inoculations, inc\"\"\")\n",
    "entities= [(i, i.label_, i.label) for i in nytimes.ents]\n",
    "displacy.render (nytimes, style = \"ent\", jupyter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3de89c",
   "metadata": {},
   "source": [
    "# Word Vector Representation\n",
    "\n",
    "A word vector is a numeric representation of a word that commuicates its relationship to other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7654f221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy. load (\"en_core_web_sm\")\n",
    "mango = n1p (u'mango')\n",
    "print (mango.vector.shape)\n",
    "print (mango.vector)  # output like : [ 1.0466383 - 1.5323697 -0.72177905 -2.4700649 -0.2715162 .....]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab7a116",
   "metadata": {},
   "source": [
    "# Computing Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aca7920",
   "metadata": {},
   "source": [
    "by percent number like 0.8252482050769769"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118aa5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy. load (\"en_core_web_sm\")\n",
    "target = nlp(\"Cats are beautiful animals.\")\n",
    "doc1 = n1p (\"Dogs are awesome.\")\n",
    "doc2 = nlp(\"Some gorgeous creatures are felines.\")\n",
    "doc3 = nlp(\"Dolphins are swimming mammals.\")\n",
    "print(target.similarity(doc1))\n",
    "print(target.similarity(doc2))\n",
    "print(target.similarity(doc3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d344a7d",
   "metadata": {},
   "source": [
    "# Get word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba7e2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "nlp = spacy. load (\"en_core_web_sm\")\n",
    "text = \"\"\"\"Most of the outlay will be at home. No surprise there, either.\n",
    "While Samsung has expanded overseas, South Korea is still host to most of its factories and research engineers.Samsung is good \"\"\"\n",
    "doc = nlp(text)\n",
    "#remove stopwords and punctuations\n",
    "words = [token.text for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "word_freq = Counter (words)\n",
    "common_words = word_freq.most_common (5)\n",
    "print (common words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d00c4b7",
   "metadata": {},
   "source": [
    "output:\n",
    "\n",
    "[ ('\\n', 2), ('Samsung', 2), ('outlay',1), ('home', 1), ('surprise', 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f69dfe",
   "metadata": {},
   "source": [
    "# Visualizing the dependency parse\n",
    "\n",
    "Depenency parsing is a language processing technique that allows us to better determine the meaning of a sentence by analyzing how it's constructed to determine how the individual words relate to each other.\n",
    "\n",
    "\n",
    "Stanford typed dependencies manual:\n",
    "    \n",
    "nsubj: nominal subject A nominal subject is a noun phrase which is the syntactic subject of a clause           \n",
    "det: determiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95d726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy. load(\"en_core _web _sm\")\n",
    "doc = nlp(u\"this is a sentence.\")\n",
    "displacy.serve (doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb724e1e",
   "metadata": {},
   "source": [
    "# Dependency Parsing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e15b0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spay. load(\"en core web sm\")\n",
    "docp = nlp (\" In pursuit of a wall, President Trump ran into one.\")\n",
    "for chunk in doc.noun chunks:\n",
    "    print(chunk. text, chunk.root.text, chunk.root.dep_,chunk. root.head. text)\n",
    "displacy.render (docp, style=\"dep\", jupyter= True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae42e3a",
   "metadata": {},
   "source": [
    "output:\n",
    "\n",
    "pursuit pursuit pobj In            \n",
    "a wall wall pobi of            \n",
    "President Trump Trump subj ran       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43afa53e",
   "metadata": {},
   "source": [
    "# Rule-based matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fff9ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy. load(\"en_core_web_sm\")\n",
    "# Matcher is initialized with the shared vocab\n",
    "from spacy. matcher import Matcher\n",
    "# Each dict represents one token and its attributes\n",
    "matcher = Matcher (nlp.vocab)\n",
    "# Add with ID, optional callback and patterns)\n",
    "pattern = [{\"LOWER\": \"new\"}, {\"LOWER\": \"york\"}]\n",
    "matcher.add( 'CITIES', None, pattern)\n",
    "# Match by calling the matcher on a Doc object\n",
    "doc = nlp(\"I live in New York\")\n",
    "matches = matcher (doc)\n",
    "# Matches are (match_id, start, end) tuples\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span by slicing the Doc\n",
    "    span = doc[ start: end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd9ab9a",
   "metadata": {},
   "source": [
    "Output:  New York"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1721755",
   "metadata": {},
   "source": [
    "# Machine Learning with text using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26e2b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn. feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords\n",
    "from sklearn. feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn. base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearsVC\n",
    "import string\n",
    "punctuations = string.punctuation\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "#Custom transformer using spaCy\n",
    "class predictors (TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        return [clean text(text) for text in X]\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# Basic utility function to clean the text\n",
    "def clean text (text):\n",
    "    return text.strip () .lower ()\n",
    "\n",
    "#Create spacy tokenizer that parses a sentence and generates tokens\n",
    "#these can also be replaced by word vectors\n",
    "def spacy tokenizer(sentence):\n",
    "    tokens = parser(sentence)\n",
    "    tokens = [tok.lemma .lower ().strip() if tok.lemma != \"-PRON-\" else tok.lower for tok in tokens]\n",
    "    tokens = [tok for tok in tokens if (tok not in stopwords and tok not in punctuations)]\n",
    "    return tokens\n",
    "\n",
    "#create vectorizer object to generate feature vectors, we will use custom spacy's tokenizer\n",
    "vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, gram_range=(1,1))\n",
    "classifier = LinearsVC()\n",
    "\n",
    "# Create the pipeline to clean, tokenize, vectorize, and classify\n",
    "pipe = Pipeline([(\"cleaner\", predictors ()),('vectorizer', vectorizer), ('classifier', classifier)])\n",
    "\n",
    "# Load sample data\n",
    "train = [('I love this sandwich.','pos'),('this is an amazing place!', 'pos'),('I feel very good about these beers.','pos'),\n",
    "         ('this is my best work.', 'pos'),('I am tired of this stuff.', 'neg'),(\"I can't deal with this\",\"neg\"),\n",
    "         ('he is my sworn enemy!','neg'),('my boss is horrible.','neg')]\n",
    "test = [('the beer was good.','pos'),('I do not enjoy my job','neg'),(\"I ain't feelin dandy today.\",'neg'),\n",
    "        (\"I feel amazing!\",'pos'),('Gary is a good friend of mine.','pos'),(\"I can't believe I'm doing this\",'neg')]\n",
    "\n",
    "# Create model and measure accuracy\n",
    "pipe.fit ([x[0] for x in train], [x[1] for x in train])\n",
    "pred_data = pipe.predict([x[0] for x in test])\n",
    "for (sample, pred) in zip(test, pred_data):\n",
    "    print (sample, pred )\n",
    "print (\"Accuracy:\",accuracy_score([x[1] for x in test], pred_data))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8b4e26",
   "metadata": {},
   "source": [
    "# Refernces:\n",
    "\n",
    "https://medium.com/@manivannan_data/spacy-named-entity-recognizer-4a1eeee1d749\n",
    "\n",
    "https://nlpforhackers.io/complete-guide-to-spacy/\n",
    "\n",
    "https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/\n",
    "\n",
    "https://spacy.io/usage/linguistic-features\n",
    "\n",
    "https://nlpforhackers.io/complete-guide-to-spacy/\n",
    "\n",
    "https://stackabuse.com/python-for-nlp-tokenization-stemming-and-lemmatization-with-spacy-library/\n",
    "\n",
    "https://nlpforhackers.io/complete-guide-to-spacy/\n",
    "\n",
    "https://towardsdatascience.com/custom-named-entity-recognition-using-spacy-7140ebbb3718\n",
    "\n",
    "https://nlpforhackers.io/complete-guide-to-spacy/\n",
    "\n",
    "https://blog.ekbana.com/nlp-for-beninners-using-spacy-6161cf48a229\n",
    "\n",
    "https://spacy.io/usage/visualizers\n",
    "\n",
    "https://www.datacamp.com/community/blog/spacy-cheatsheet\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2017/04/natural-language-processing-made-easy-using-spacy-%E2%80%8Bin-python/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec8f46f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
