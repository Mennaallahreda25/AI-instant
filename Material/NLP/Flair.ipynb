{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "968dd368",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e99314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "# Make a sentence object by passing an untokenized\n",
    "#string and the 'use tokenizer' flag\n",
    "sentence = Sentence ('The grass is green.', use_tokenizer=True)\n",
    "# Print the object to see what's in there\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272595d8",
   "metadata": {},
   "source": [
    "Sentence: \"The grass is green .\" - 5 Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f8e7e6",
   "metadata": {},
   "source": [
    "# Creating a Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eee5d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sentence objects holds a sentence that we may want to embed or tag\n",
    "from flair.data import Sentence\n",
    "# Make a sentence object by passing a whitespace tokenized string\n",
    "sentence = Sentence ('The grass is green .')\n",
    "# Print the object to see what's in there\n",
    "print(sentence)\n",
    "# using the token id\n",
    "print(sentence.get token (4))\n",
    "# using the index itself\n",
    "print(sentence [3])\n",
    "for token in sentence:\n",
    "    print (token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1b37ce",
   "metadata": {},
   "source": [
    "Output:                   \n",
    "Sentence: \"The grass is green.\" - 5 Tokens                          \n",
    "Token: 4 green                         \n",
    "Token: 4 green                             \n",
    "Token: 1 The Token: 2 grass                      \n",
    "Token: 3 is                          \n",
    "Token: 4 green                 \n",
    "Token: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67b7dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sentence obiects holds a sentence\n",
    "from flair.data import Sentence\n",
    "# Make a sentence object by passing a whitespace\n",
    "sentence = Sentence(\"انا احب لغة بايثون\")\n",
    "# Print the object to see what's in there\n",
    "print(sentence)\n",
    "# using the token id\n",
    "print (sentence. get_token (4))\n",
    "# using the index itself\n",
    "print (sentence [3])\n",
    "for token in sentence:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb90639",
   "metadata": {},
   "source": [
    "Output:            \n",
    "Sentence: \"انا احب لغة بايثون\" - 4 Tokens                                       \n",
    "Token: 4 بايثون                            \n",
    "Token: 4 بايثون                        \n",
    "Token: 1 انا                   \n",
    "Token: 3 احب              \n",
    "Token: 4 لغة           \n",
    "Token: 5 بايثون                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e85b45",
   "metadata": {},
   "source": [
    "# Adding Tags to Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2abe445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "# Make a sentence object by passing an untokenized\n",
    "#string and the 'use tokenizer' flag\n",
    "sentence = Sentence ('The grass is green.', use_tokenizer=True)\n",
    "# Print the object to see what's in there\n",
    "print (sentence)\n",
    "# add a tag to a word in the sentence\n",
    "sentence [3].add_tag('ner', 'color')\n",
    "# print the sentence with all tags of this type\n",
    "print (sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feaf07e",
   "metadata": {},
   "source": [
    "Sentence: \"The grass is green .\" - 5 Tokens\n",
    "The grass is green <color>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c000fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.data import Label\n",
    "# Make a sentence object by passing an untokenized\n",
    "#string and the 'use tokenizer' flag\n",
    "sentence = Sentence('The grass is green.', use_tokenizer=True)\n",
    "# Print the object to see what's in there\n",
    "print (sentence)\n",
    "# add a tag to a word in the sentence\n",
    "sentence [3].add_tag('ner','color')\n",
    "# print the sentence with all tags of this type\n",
    "print(sentence.to tagged_string())\n",
    "tag: Label = sentence [3]. get_tag('ner')\n",
    "print (f'\" {sentence [3]}\" is tagged as \"{tag.value}\" with confidence score \"{tag.score}\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fee552",
   "metadata": {},
   "source": [
    "Sentence: \"The grass is green\n",
    "- 5 Tokens\n",
    "The grass is green <colors .\n",
    "\"Token: 4 green\" is tagged as \"color\" with confidence score \"1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32a6ce9",
   "metadata": {},
   "source": [
    "# Adding Labels to Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b5ac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "sentence = Sentence ('France is the current world cup winner.')\n",
    "# add a label to a sentence\n",
    "sentence.add label('sports')\n",
    "# a sentence can also belong to multiple classes\n",
    "sentence.add_labels (['sports', 'world cup'])\n",
    "# you can also set the labels while initializing the sentence\n",
    "sentence = Sentence ('France is the current world cup winner.', labels=['sports', 'world cup'])\n",
    "print (sentence)\n",
    "for label in sentence.labels:\n",
    "    print (label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccd30d5",
   "metadata": {},
   "source": [
    "Sentence: \"France is the current world cup winner.\" - 7 Tokens -                           \n",
    "Labels: [sports (1.0), world cup (1.0)] sports (1.0) world cup (1.0)\n",
    "\n",
    "-This indicates that the sentence belongs to these two classes, each with confidence score 1.0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbe90fe",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)\n",
    "\n",
    "- What is Named Entity Recognition (NER)?\n",
    "Named entity recognition (NER) is a sub-task of information extraction (IE) that seeks out and categories specified entities in a body or bodies of texts.                   \n",
    "\n",
    "The dataset consists of the following tags:\n",
    "•geo = Geographical Entity                  \n",
    "•org = Organization                 \n",
    "•per = Person                     \n",
    "•gpe = Geopolitical Entity             \n",
    "•tim = Time indicator                          \n",
    "•art = Artifact                       \n",
    "•eve = Event                          \n",
    "•pat = Natural Phenomenon             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a60c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence\n",
    "tagger = SequenceTagger. load('ner')\n",
    "sentence = Sentence ('George Washington went to Washington .')\n",
    "# predict NER tags\n",
    "tagger.predict (sentence)\n",
    "# print sentence with predicted tags\n",
    "print(sentence.to_tagged_string())\n",
    "for entity in sentence.get_spans ('ner'):\n",
    "    print (entity)\n",
    "print (sentence.to dict(tag( type='ner'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c237333",
   "metadata": {},
   "source": [
    "George <B-PER> Washington <E-PER> went to Washington <S- LOC>                       \n",
    "PER-span [1,2]: \"George Washington\"                                     \n",
    "LOC-span [5]: \"Washington\"                              \n",
    "{'text': 'George Washington went to Washington .',\n",
    "'labels': [], 'entities': [{'text': 'George\n",
    "Washington'\n",
    "\"start pos': 0,\n",
    "'end pos': 17,\n",
    "\"type': 'PER',\n",
    "'confidence': 0.9967882037162781}, {'text'\n",
    "'Washington\n",
    "'start_pos': 26, 'end_pos': 36, 'type': 'LOC', 'confidence': 0.9993709921836853}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711c63b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "# make a sentence\n",
    "sentence = Sentence ('I love Berlin .')\n",
    "# load the NER tagger\n",
    "tagger = SequenceTagger. load('ner')\n",
    "# run NER over sentence\n",
    "tagger.predict (sentence)\n",
    "print (sentence)\n",
    "print ('The following NER tags are found:')\n",
    "# iterate over entities and print\n",
    "for entity in sentence.get_spans ('ner'):\n",
    "    print (entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f8bcf2",
   "metadata": {},
   "source": [
    "Sentence: \"I love Berlin .\" - 4 Tokens                  \n",
    "The following NER tags are found:                    \n",
    "LOC-span [3]: \"Berlin\"               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d84e6a7",
   "metadata": {},
   "source": [
    "# Text Classification and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07a604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import Textclassifier\n",
    "from flair.data import Sentence\n",
    "classifier = TextClassifier. load('en-sentiment')\n",
    "sentence = Sentence ('This film hurts. It is so bad that I am confused.')\n",
    "# predict NER tags\n",
    "classifier.predict(sentence)\n",
    "# print sentence with predicted labels\n",
    "print (sentence.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22aac23",
   "metadata": {},
   "source": [
    "[NEGATIVE (1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7e70ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "classifier = TextClassifier.load( 'en-sentiment')\n",
    "sentence = Sentence ('Flair is pretty neat!')\n",
    "classifier.predict(sentence)\n",
    "# print sentence with predicted labels\n",
    "print( 'Sentence above is: ', sentence.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a952b5c",
   "metadata": {},
   "source": [
    "Sentence above is: \n",
    "[POSITIVE (0.8746314644813538)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e70bcf",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058da4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import WordEmbeddings\n",
    "from flair.data import Sentence\n",
    "# init embedding\n",
    "glove_embedding = WordEmbeddings ('glove')\n",
    "# create sentence.\n",
    "sentence = Sentence ('The grass is green .')\n",
    "# embed a sentence using glove.\n",
    "glove_embedding.embed (sentence)\n",
    "# now check out the embedded tokens.\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print (token.embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd15905",
   "metadata": {},
   "source": [
    "Token: 1 The\n",
    "tensor([-0.0382, -0.2449,\n",
    "0.7281, -0.3996,\n",
    "-0.5755, 0.0875, 0.2879, -0.0673,\n",
    "0.3340, -0.3385, -0.3174, -0.4834,\n",
    "0.4495, -0.4697,\n",
    "0.0263,\n",
    "-0.5415,\n",
    "0.1439, 0.2346, -0.3102, 0.0862,\n",
    "-0.7179, -0.4153,\n",
    "0.2033, -0.1276,\n",
    "-0.3656, -0.5486, -0.0629,\n",
    "0.0162, -0.0171,\n",
    "-0.5203, -0.1459, 0.8278, 0.2706])                       \n",
    "Token: 2 grass tensor([-0.8135,\n",
    "0.9404, -0.2405, -0.1350,\n",
    "-0.5478, -0.3537, 0.0734, 0.2587,\n",
    "0.1950,\n",
    "0.5346,\n",
    "0.6166,\n",
    "0.7424,\n",
    "0.7284,\n",
    "0.0578,\n",
    "-0.3262, -1.3641,\n",
    "1.0822, -0.2296, 0.6039, 0.5541,\n",
    "-0.1637, -0.8468, 0.0741, -0.6216,\n",
    "-0.0161, -0.4972, -0.5534, -0.4037,\n",
    "0.4928, 0.9488,\n",
    "0.2040,\n",
    "2.3436, -0.2207, 8:886, -8.6927.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26137db5",
   "metadata": {},
   "source": [
    "# Document Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a4484b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import WordEmbeddings, DocumentRNNEmbeddings\n",
    "from flair.data import Sentence\n",
    "glove_ embedding = WordEmbeddings('glove')\n",
    "document_embeddings = DocumentRNNEmbeddings ([glove_embedding])\n",
    "# create an example sentence\n",
    "sentence = Sentence ('The grass is green . And the sky is blue .')\n",
    "# embed the sentence with our document embedding\n",
    "document_ embeddings.embed(sentence)\n",
    "# now check out the embedded sentence.\n",
    "print (sentence.get_embedding())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2458c169",
   "metadata": {},
   "source": [
    "tensor([-0.0382, -0.2449, 0.7281, -0.3996, -0.5755, 0.0875, 0.2879, -0.0673, 0.3340, -0.3385, -0.3174, -0.4834, 0.4495, -0.4697, 0.0263, -0.5415, 0.1439, 0.2346, -0.3102, 0.0862, -0.7179, -0.4153, 0.2033, -0.1276, -0.3656, -0.5486, -0.0629, 0.0162, -0.0171, -0.5203, -0.1459, 0.8278, 0.2706])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd282e6",
   "metadata": {},
   "source": [
    "# Loading Training Data\n",
    "\n",
    "The Corpus Object:                         \n",
    "The Corpus represents a dataset that you use to train a model. It consists of a list of train sentences, a list of dev sentences, and a list of test sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb72d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flair.datasets\n",
    "corpus = flair.datasets.UD_ENGLISH()\n",
    "# print the number of Sentences in the train split\n",
    "print (len (corpus.train))\n",
    "# print the number of Sentences in the test split\n",
    "print(len (corpus.test))\n",
    "# print the number of Sentences in the dev split\n",
    "print(len (corpus .dev))\n",
    "# print the first Sentence in the training split\n",
    "print(corpus.test [0])\n",
    "# print the first Sentence in the training split\n",
    "print (corpus.test[0].to_tagged_string('pos'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ea8a77",
   "metadata": {},
   "source": [
    "1. Flair datasets \\ud english en ewt-ud-test. conflu                     \n",
    "2019-07-26 03:02:17,588 removing temp file C: \\Users \\hhourani \\AppData\\Local\\T                         \n",
    "2019-07-26 03:02:20,727 https://raw.githubusercontent.com/UniversalDependend train.conllu not found in cache, downloading to C: \\Users\\hourani\\AppData\\Lo\n",
    "13303045B [00:06, 2072067.11B/s]                             \n",
    "2019-07-26 03:02:27,961 copying C: \\Users\\hhourani \\AppData\\Local\\Temp\\tmp12km\n",
    "I. flair\\datasets \\ud_englishlen_ewt-ud-train.conllu\n",
    "2019-07-26 03:02:27,981 removing temp file C: \\Users \\hhourani\\AppData\\Local\\T\n",
    "2019-07-26 03:02:27,985 Reading data from C: \\Users \\houranil.flair\\datasets\\\n",
    "2019-07-26 03:02:27,985 Train: C: \\Users\\hhourani\\.flair\\datasets\\ud_english\\\n",
    "2019-07-26 03:02:27,985 Test: C: \\Users\\hhourani\\.flair\\datasets\\ud_englishle\n",
    "2019-07-26 03:02:27,985 Dev: C: \\Users \\hhouranil.flair\\datasets\\ud_english\\en\n",
    "12543\n",
    "2077\n",
    "2002\n",
    "Sentence: \"What if Google Morphed Into Googles ?\" - 7 Tokens                          \n",
    "What <W> if <IN> Google <NNP> Morphed <VBD> Into <IN› GoogleS <NNP> ? <.>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d0fd8f",
   "metadata": {},
   "source": [
    "# Classify Spam\n",
    "\n",
    "Using Transfer Learning and Pre-trained Language Models to Classify Spam link:                         \n",
    "https://heartbeat.fritz.ai/using-transfer-learning-and-pre-trained-language-models-to-classify-spam-549c0f56c20\n",
    "\n",
    "Downloading SMSSpamCollection.txt link:\n",
    "http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef52e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMSSpamCollection.txt\n",
    "ham       Go until jurong point, crazy.\n",
    "ham       ok lar...Joking wif u oni.                     \n",
    "spam      Free entry in 2 a wkly comp to           \n",
    "\n",
    "\n",
    "# Train the model\n",
    "Dataset\n",
    "Word Embedding\n",
    "Document Embedding\n",
    "Classifier\n",
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dbd08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "from flair.data_fetcher import NLPTaskDataFetcher\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentLSTMEmbeddings\n",
    "from flair.models import Textclassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from pathlib import Path\n",
    "from flair.data import Sentence\n",
    "\n",
    "# Loading and Pre-processing the Data\n",
    "data = pd.read csv('SMSSpamCollection.txt',delimiter='\\t',header=None)\n",
    "data = data.rename (columns={0:\"label\", 1:\"text\"}).drop duplicates ()\n",
    "data['label '] = 'label' + data['label '1.astype(str)\n",
    "data. iloc [0: int (len (data)*0.8)].to_csv('train.csv', sep='\\t',index = False, header = False)\n",
    "data.iloc[int (len (data)*0.8) :int (len (data)*0.9)].to_csv('test.csv', sep='10',index = False, header = False)\n",
    "data.iloc[ int(len (data)*0.9):].to_csv('dev.csv', sep='\\t', index = False, header = False)\n",
    "                                \n",
    "# Train the model\n",
    "corpus = NLPTaskDataFetcher.load_classification_corpus(Path('./'), test_file= 'test.csv',dev_file='dev.csv', train_file='train.csv')\n",
    "word embeddings = [WordEmbeddings ('glove'),FlairEmbeddings ('news-forward-fast'), FlairEmbeddings ('news-backward-fast')]\n",
    "document_embeddings = DocumentLSTMEmbeddings (word_embeddings, hidden_size=512,reproject_words=True, reproject_words_dimension=256)\n",
    "classifier = TextClassifier (document_embeddings,label_dictionary=corpus.make_label_dictionary (), multi_label=False)\n",
    "trainer = ModelTrainer (classifier, corpus)\n",
    "trainer.train('./', max epochs=10)\n",
    "                                \n",
    "#predict\n",
    "classifier = TextClassifier. load('/best-model.pt')\n",
    "# sentence = data['text'].tolist()\n",
    "sent = [\"FREE entry into our £250 weekly comp just \\\n",
    "send the word WIN to 80086 NOW. 18 T&C www.txttowin.co.uk\"]\n",
    "sentence = Sentence (sent)\n",
    "classifier.predict(sentence)\n",
    "# print (sentence.labels)\n",
    "label = str (sentence. labels [0]) .split() [0]\n",
    "print (f\"{label}\\t{sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a665f248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "classifier = TextClassifier. load('./best-model.pt')\n",
    "# sentence = data['text'].tolist ()\n",
    "sent = [\"FREE entry into our £250 weekly comp just \\\n",
    "send the word WIN to 80086 NOW. 18 T&C www.txttowin.co.uk\"]\n",
    "sentence = Sentence (sent)\n",
    "classifier.predict(sentence)\n",
    "# print(sentence. labels)\n",
    "label = str(sentence.labels[0]).split()[0]\n",
    "print (f\"{label}\\t{sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbf07a6",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "2019-09-28 22:05:06,990 loading file ./best-model.pt\n",
    "C: \\Python37New\\lib\\site-packages\\torch\\serialization.p:574: DeprecationWarning: Call to deprecated class DocumentLSTMEmbeddings. (The functionality of this class is moved to\n",
    "'DocumentRNNEmbeddings') -- Deprecated since version 0.4.\n",
    "result = unpickler. load()\n",
    "spam\n",
    "Sentence: \"FREE entry into our £250 weekly comp just\n",
    "send the word WIN to\n",
    "80086 Now. 18 T&C www.txttowin.co.uk\" - 1 Tokens - Labels: [spam (0.8216946125030518)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb33a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "classifier = TextClassifier. load('./best-model.pt')\n",
    "# sentence = data[ 'text'].tolist)\n",
    "sent = [\"Hi Hussam, How are you?\"]\n",
    "sentence = Sentence(sent)\n",
    "classifier.predict(sentence)\n",
    "# print (sentence.labels)\n",
    "label = str (sentence. labels [0]).split()[0]\n",
    "print (f\"{label}\\t{sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9c6dcb",
   "metadata": {},
   "source": [
    "Output: \n",
    "    \n",
    "2019-09-28 22:12:08,972 loading file ./best-model.pt\n",
    "C: \\Python37New\\lib\\site-packages\\torch\\serialization.p:574: DeprecationWarning: Call to deprecated class DocumentLSTMEmbeddings. (The functionality of this class is moved to\n",
    "\"DocumentRNNEmbeddings') - - Deprecated since version 0.4.\n",
    "result = unpickler. load()\n",
    "ham\n",
    "Sentence: \"Hi Hussam, How are you?\" - 1 Tokens - Labels: [ham (0.8704296946525574)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e2f215",
   "metadata": {},
   "source": [
    "# Classify Spam using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0525e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn. feature_extraction.text import TfidfVectorizer\n",
    "from sklearn. linear_model.logistic import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "#read the data\n",
    "df = pd.read csv('SMSSpamCollection.txt', delimiter='\\t',header=None)\n",
    "df.rename (columns = {0: 'label',1: 'text'}, inplace = True)\n",
    "#Input and output variables\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "seed = 5\n",
    "test_size = 0.33\n",
    "#split dataset into train and test sets\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, Y,test_size=test_size,random_state=seed)\n",
    "#Convert to a matrix of TF-IDF features\n",
    "vectorizer = TfidfVectorizer ()\n",
    "x_train = vectorizer.fit_transform(X_train_raw)\n",
    "X_test = vectorizer.transform(X_test_raw)\n",
    "#Model training\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit (X_train, y_train)\n",
    "#prediction\n",
    "predictions = classifier.predict (X_test)\n",
    "#model evaluation\n",
    "score = accuracy_score (y_test, predictions)\n",
    "f_score = f1_score (_test, predictions, average='micro')\n",
    "print(\"The accuracy score (Logistic Regression) is:\" , score)\n",
    "print(\"The F score-Micro (Logistic Regression) is:\" , f_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147270d2",
   "metadata": {},
   "source": [
    "C: \\Python37New\\lib\\site-packages\\sklearn\\linear model\\logistic. solver will be changed to 'Ibfgs' in 0.22. Specify a solver to FutureWarning)\n",
    "The accuracy score (Logistic Regression) is: 0.9668297988036977                       \n",
    "The r score-Micro (Logistic Regression) Is: 0.966829798803697\n",
    "\n",
    "Flair's Score:                                \n",
    "EPOCH 7 done: loss 0.0427 - Ir 0 1000\n",
    "DEV : loss 0.04332328587770462 - score 0.9903\n",
    "BAD EPOCHS (no improvement): 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bebf0f",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_1_BASICS.md\n",
    "\n",
    "https://github.com/zalandoresearch/flair\n",
    "\n",
    "https://heartbeat.fritz.ai/using-transfer-learning-and-pre-trained-language-models-to-classify-spam-549c0f56c20\n",
    "\n",
    "http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/\n",
    "\n",
    "https://pythonprogramming.net/sentiment-analysis-python-textblob-vader/\n",
    "\n",
    "https://textblob.readthedocs.io/en/dev/quickstart.html\n",
    "\n",
    "https://textblob.readthedocs.io/en/dev/advanced_usage.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5520cba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
