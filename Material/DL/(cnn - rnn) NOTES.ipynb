{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce44e39e",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53739ca",
   "metadata": {},
   "source": [
    "# Activation functions in Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeeae6f",
   "metadata": {},
   "source": [
    "It is recommended to understand Neural Networks before reading this article. \n",
    "\n",
    "In the process of building a neural network, one of the choices you get to make is what Activation Function to use in the hidden layer as well as at the output layer of the network. This article discusses some of the choices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370d570a",
   "metadata": {},
   "source": [
    "What is an activation function and why use them? \n",
    "The activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it. The purpose of the activation function is to introduce non-linearity into the output of a neuron. \n",
    "\n",
    "Explanation: We know, the neural network has neurons that work in correspondence with weight, bias, and their respective activation function. In a neural network, we would update the weights and biases of the neurons on the basis of the error at the output. This process is known as back-propagation. Activation functions make the back-propagation possible since the gradients are supplied along with the error to update the weights and biases. \n",
    "\n",
    "Why do we need Non-linear activation function?\n",
    "A neural network without an activation function is essentially just a linear regression model. The activation function does the non-linear transformation to the input making it capable to learn and perform more complex tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad79e757",
   "metadata": {},
   "source": [
    "1- ReLU Function Formula\n",
    "There are a number of widely used activation functions in deep learning today. One of the simplest is the rectified linear unit, or ReLU function, which is a piecewise linear function that outputs zero if its input is negative, and directly outputs the input otherwise:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e0cf64",
   "metadata": {},
   "source": [
    "2-ReLU Function Derivative\n",
    "It is also instructive to calculate the gradient of the ReLU function, which is mathematically undefined at x = 0 but which is still extremely useful in neural networks.\n",
    "\n",
    "\n",
    "\n",
    "The derivative of the ReLU function. In practice the derivative at x = 0 can be set to either 0 or 1. \n",
    "\n",
    "The zero derivative for negative x\n",
    "\n",
    " can give rise to problems when training a neural network, since a neuron can become 'trapped' in the zero region and backpropagation will never change its weights.\n",
    "\n",
    "3-PReLU Function, a Variation on the ReLU\n",
    "Because of the zero gradient problem faced by the ReLU, it is sometimes common to use an adjusted ReLU function called the parametric rectified linear unit, or PReLU:\n",
    "\n",
    "\n",
    "\n",
    "This has the advantage that the gradient is nonzero at all points (except 0 where it is undefined).\n",
    "\n",
    "4-Logistic Sigmoid Function Formula\n",
    "Another well-known activation function is the logistic sigmoid function:\n",
    "\n",
    "\n",
    "Mathematical definition of the Logistic Sigmoid Function\n",
    "\n",
    "The logistic sigmoid function has the useful property that its gradient is defined everywhere, and that its output is conveniently between 0 and 1 for all x. The logistic sigmoid function is easier to work with mathematically, but the exponential functions make it computationally intensive to compute in practice and so simpler functions such as ReLU are often preferred.\n",
    "\n",
    "5-Logistic Sigmoid Function Derivative\n",
    "The derivative of the logistic sigmoid function can be expressed in terms of the function itself:\n",
    "\n",
    "The derivative of the logistic sigmoid function.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Graph of the derivative of the logistic sigmoid function\n",
    "\n",
    "The derivative of the logistic sigmoid function is nonzero at all points, which is an advantage for use in the backpropagation algorithm, although the function is intensive to compute, and the gradient becomes very small for large absolute x, giving rise to the vanishing gradient problem.\n",
    "\n",
    "Because the derivative contains exponentials, it is computationally expensive to calculate. The backpropagation algorithm requires the derivative of all operations in a neural network to be calculated, and so the sigmoid function is not well suited for use in neural networks in practice due to the complexity of calculating its derivative repeatedly.\n",
    "\n",
    "\n",
    "6-Activation Function vs Action Potential\n",
    "Although the idea of an activation function is directly inspired by the action potential in a biological neural network, there are few similarities between the two mechanisms.\n",
    "\n",
    "Biological action potentials do not return continuous values. A biological neuron is either triggered or it is not, rather like the threshold step function. There is no possibility for a continuously varying output, as is normally required in order for backpropagation to be possible.\n",
    "\n",
    "Furthermore, an action potential is a function in time. When the voltage across a neuron passes a threshold value, the neuron will trigger, and deliver a characteristic curve of output voltage to its neighbors over the next few milliseconds, before returning to its rest state.\n",
    "\n",
    "\n",
    "The shape of a typical action potential over time as a signal passes a point on a cell membrane\n",
    "\n",
    "Learning in the human brain runs asynchronously, with each neuron operating independently from the rest, and being only connected to its immediate neighbors. On the other hand, the \"neurons\" in an artificial neural network are often connected to thousands or millions of other neurons, and are executed by the processor in a defined order.\n",
    "\n",
    "Although biological neurons cannot return continuous values, because of the added time dimension, information is transmitted in the firing frequency and firing mode of a neuron.\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "# Applications of the Activation Function\n",
    "Activation Function in Artificial Neural Networks:\n",
    "Activation functions are an essential part of an artificial neural network. They enable a neural network to be built by stacking layers on top of each other, glued together with activation functions. Usually, each layer consists of a function that multiplies the input by a weight and adds a bias, followed by an activation function and then the next weight and bias.\n",
    "\n",
    "A simple feedforward neural network with activation functions following each weight and bias operation.\n",
    "\n",
    "Each node and activation function pair outputs a value of the form\n",
    "\n",
    "\n",
    "\n",
    "where g is the activation function, W is the weight at that node, and b is the bias. The activation function g could be any of the activation functions listed so far. In fact, g could be nearly any nonlinear function.\n",
    "\n",
    "Ignoring vector values for simplicity, a neural network with two hidden layers can then be written as\n",
    "\n",
    "\n",
    "Given that g could be any nonlinear function, there is no way to simplify this expression further. The expression represents the simplest form of the computational graph of the entire neural network.\n",
    "\n",
    "A simple neural network like this is capable of learning many complicated logical or arithmetic functions, such as the XOR function.\n",
    "\n",
    "However, let us imagine removing the activation functions from the network.\n",
    "\n",
    "The network formula can now be simplified to a simple linear equation in x:\n",
    "\n",
    "\n",
    "\n",
    "Therefore, a neural network with no activation functions is equivalent to a linear regression model, and can perform no operation more complex than linear modeling.\n",
    "\n",
    "For this reason, all modern neural networks use a kind of activation function.\n",
    "\n",
    "# Activation Function in the Single Layer Perceptron\n",
    "Taking the concept of the activation function to first principles, a single neuron in a neural network followed by an activation function can behave as a logic gate.\n",
    "\n",
    "Let us take the threshold step function as our activation function:\n",
    "\n",
    "\n",
    "\n",
    "Mathematical definition of the threshold step function, one of the simplest possible activation functions\n",
    "\n",
    "The threshold step function may have been the first activation function, introduced by Frank Rosenblatt while he was modeling biological neurons in 1962.\n",
    "\n",
    "And let us define a single layer neural network, also called a single layer perceptron, as:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Formula and computational graph of a simple single-layer perceptron with two inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab859d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T00:26:45.416499Z",
     "start_time": "2023-03-14T00:26:45.399048Z"
    }
   },
   "source": [
    "##############################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2471350c",
   "metadata": {},
   "source": [
    "# Types of CNN Models.\n",
    "\n",
    "2.1 LeNet\n",
    "2.2 AlexNet\n",
    "2.3 ResNet\n",
    "2.4 GoogleNet/InceptionNet\n",
    "2.5 MobileNetV1\n",
    "2.6 ZfNet\n",
    "2.7 Depth based CNNs\n",
    "2.8 Highway Networks\n",
    "2.9 Wide ResNet\n",
    "2.10 VGG\n",
    "2.11 PolyNet\n",
    "2.12 Inception v2\n",
    "2.13 Inception v3 V4 and Inception-ResNet.\n",
    "2.14 DenseNet\n",
    "2.15 Pyramidal Net\n",
    "2.16 Xception\n",
    "2.17 Channel Boosted CNN using TL\n",
    "2.18 Residual Attention NN\n",
    "2.19 Attention Based CNNS\n",
    "2.20 Feautre-Map based CNNS\n",
    "2.21 Squeeze and Excitation Networks\n",
    "2.22 Competitive Squeeze and Excitation Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345942b5",
   "metadata": {},
   "source": [
    "# Layer Types\n",
    "\n",
    "There are many types of layers used to build Convolutional Neural Networks, but the ones you are most likely to encounter include:\n",
    "\n",
    "Convolutional (CONV)\n",
    "Activation (ACT or RELU, where we use the same or the actual activation function)\n",
    "Pooling (POOL)\n",
    "Fully connected (FC)\n",
    "Batch normalization (BN)\n",
    "Dropout (DO)\n",
    "\n",
    "Stacking a series of these layers in a specific manner yields a CNN. We often use simple text diagrams to describe a CNN: INPUT => CONV => RELU => FC => SOFTMAX.\n",
    "\n",
    "Here, we define a simple CNN that accepts an input, applies a convolution layer, then an activation layer, then a fully connected layer, and, finally, a softmax classifier to obtain the output classification probabilities. The SOFTMAX activation layer is often omitted from the network diagram as it is assumed it directly follows the final FC.\n",
    "\n",
    "Of these layer types, CONV and FC (and to a lesser extent, BN) are the only layers that contain parameters that are learned during the training process. Activation and dropout layers are not considered true “layers” themselves but are often included in network diagrams to make the architecture explicitly clear. Pooling layers (POOL), of equal importance as CONV and FC, are also included in network diagrams as they have a substantial impact on the spatial dimensions of an image as it moves through a CNN.\n",
    "\n",
    "CONV, POOL, RELU, and FC are the most important when defining your actual network architecture. That’s not to say that the other layers are not critical, but take a backseat to this critical set of four as they define the actual architecture itself.\n",
    "\n",
    "Remark: Activation functions themselves are practically assumed to be part of the architecture, When defining CNN architectures we often omit the activation layers from a table/diagram to save space; however, the activation layers are implicitly assumed to be part of the architecture.\n",
    "\n",
    "In this tutorial, we’ll review each of these layer types in detail and discuss the parameters associated with each layer (and how to set them). In a future tutorial, I’ll discuss in more detail how to stack these layers properly to build your own CNN architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5cec4a",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb3fe7a",
   "metadata": {},
   "source": [
    "tf.keras.layers.Embedding(\n",
    "    input_dim,\n",
    "    output_dim,\n",
    "    embeddings_initializer=\"uniform\",\n",
    "    embeddings_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    embeddings_constraint=None,\n",
    "    mask_zero=False,\n",
    "    input_length=None,\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "التضمين\n",
    "odel.add(tf.keras.layers.Embedding(1000, 64, input_length=10))\n",
    ">>#input_dim=1000, output_dim=64)\n",
    "model = Sequential() model.add(Embedding(10000, embedding_size, input_length=max_words)\n",
    ">>> # The model will take as input an integer matrix of size (batch,\n",
    ">>> # input_length), and the largest integer (i.e. word index) in the input\n",
    ">>> # should be no larger than 999 (vocabulary size).\n",
    ">>> # Now model.output_shape is (None, 10, 64), where `None` is the batch\n",
    ">>> # dimension.\n",
    "\n",
    "Arguments\n",
    "\n",
    "input_dim: Integer. Size of the vocabulary, i.e. maximum integer index + 1.\n",
    "output_dim: Integer. Dimension of the dense embedding.\n",
    "embeddings_initializer: Initializer for the embeddings matrix (see keras.initializers).\n",
    "embeddings_regularizer: Regularizer function applied to the embeddings matrix (see keras.regularizers).\n",
    "embeddings_constraint: Constraint function applied to the embeddings matrix (see keras.constraints).\n",
    "mask_zero: Boolean, whether or not the input value 0 is a special \"padding\" value that should be masked out. This is useful when using recurrent layers which may take variable length input. If this is True, then all subsequent layers in the model need to support masking or an exception will be raised. If mask_zero is set to True, as a consequence, index 0 cannot be used in the vocabulary (input_dim should equal size of vocabulary + 1).\n",
    "input_length: Length of input sequences, when it is constant. This argument is required if you are going to connect Flatten then Dense layers upstream (without it, the shape of the dense outputs cannot be computed).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f27b804",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "In such cases, you should place the embedding matrix on the CPU memory. You can do so with a device scope, as such:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41d5c091",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T03:19:13.479773Z",
     "start_time": "2023-03-14T03:19:13.212968Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"with tf.device('cpu:0'):\\n  embedding_layer = Embedding(...)\\n  embedding_layer.build()\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"with tf.device('cpu:0'):\n",
    "  embedding_layer = Embedding(...)\n",
    "  embedding_layer.build()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3339ebb5",
   "metadata": {},
   "source": [
    " \n",
    "  \n",
    "The pre-built embedding_layer instance can then be added to a Sequential model (e.g. model.add(embedding_layer)), \n",
    "called in a Functional model (e.g. x = embedding_layer(x)), or used in a subclassed model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995b364d",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "An embedding is a mapping of a discrete — categorical — variable to a vector of continuous numbers. In the context of neural networks, embeddings are low-dimensional, learned continuous vector representations of discrete variables. Neural network embeddings are useful because they can reduce the dimensionality of categorical variables and meaningfully represent categories in the transformed space.\n",
    "\n",
    "Neural network embeddings have 3 primary purposes:\n",
    "\n",
    "Finding nearest neighbors in the embedding space. These can be used to make recommendations based on user interests or cluster categories.\n",
    "As input to a machine learning model for a supervised task.\n",
    "For visualization of concepts and relations between categories.\n",
    "This means in terms of the book project, using neural network embeddings, we can take all 37,000 book articles on Wikipedia and represent each one using only 50 numbers in a vector. Moreover, because embeddings are learned, books that are more similar in the context of our learning problem are closer to one another in the embedding space.\n",
    "\n",
    "Neural network embeddings overcome the two limitations of a common method for representing categorical variables: one-hot encoding.\n",
    "\n",
    "Limitations of One Hot Encoding\n",
    "The operation of one-hot encoding categorical variables is actually a simple embedding where each category is mapped to a different vector. This process takes discrete entities and maps each observation to a vector of 0s and a single 1 signaling the specific category.\n",
    "\n",
    "The one-hot encoding technique has two main drawbacks:\n",
    "\n",
    "For high-cardinality variables — those with many unique categories — the dimensionality of the transformed vector becomes unmanageable.\n",
    "The mapping is completely uninformed: “similar” categories are not placed closer to each other in embedding space.\n",
    "The first problem is well-understood: for each additional category — referred to as an entity — we have to add another number to the one-hot encoded vector. If we have 37,000 books on Wikipedia, then representing these requires a 37,000-dimensional vector for each book, which makes training any machine learning model on this representation infeasible.\n",
    "\n",
    "The second problem is equally limiting: one-hot encoding does not place similar entities closer to one another in vector space. If we measure similarity between vectors using the cosine distance, then after one-hot encoding, the similarity is 0 for every comparison between entities.\n",
    "\n",
    "This means that entities such as War and Peace and Anna Karenina (both classic books by Leo Tolstoy) are no closer to one another than War and Peace is to The Hitchhiker’s Guide to the Galaxy if we use one-hot encoding.\n",
    "\n",
    "# One Hot Encoding Categoricals\n",
    "books = [\"War and Peace\", \"Anna Karenina\", \n",
    "          \"The Hitchhiker's Guide to the Galaxy\"]\n",
    "books_encoded = [[1, 0, 0],\n",
    "                 [0, 1, 0],\n",
    "                 [0, 0, 1]]\n",
    "Similarity (dot product) between First and Second = 0\n",
    "Similarity (dot product) between Second and Third = 0\n",
    "Similarity (dot product) between First and Third = 0\n",
    "Considering these two problems, the ideal solution for representing categorical variables would require fewer numbers than the number of unique categories and would place similar categories closer to one another.\n",
    "\n",
    "# Idealized Representation of Embedding\n",
    "books = [\"War and Peace\", \"Anna Karenina\", \n",
    "          \"The Hitchhiker's Guide to the Galaxy\"]\n",
    "books_encoded_ideal = [[0.53,  0.85],\n",
    "                       [0.60,  0.80],\n",
    "                       [-0.78, -0.62]]\n",
    "Similarity (dot product) between First and Second = 0.99\n",
    "Similarity (dot product) between Second and Third = -0.94\n",
    "Similarity (dot product) between First and Third = -0.97\n",
    "To construct a better representation of categorical entities, we can use an embedding neural network and a supervised task to learn embeddings.\n",
    "\n",
    "Learning Embeddings\n",
    "The main issue with one-hot encoding is that the transformation does not rely on any supervision. We can greatly improve embeddings by learning them using a neural network on a supervised task. The embeddings form the parameters — weights — of the network which are adjusted to minimize loss on the task. The resulting embedded vectors are representations of categories where similar categories — relative to the task — are closer to one another.\n",
    "\n",
    "For example, if we have a vocabulary of 50,000 words used in a collection of movie reviews, we could learn 100-dimensional embeddings for each word using an embedding neural network trained to predict the sentimentality of the reviews. (For exactly this application see this Google Colab Notebook). Words in the vocabulary that are associated with positive reviews such as “brilliant” or “excellent” will come out closer in the embedding space because the network has learned these are both associated with positive reviews.\n",
    "\n",
    "\n",
    "Movie Sentiment Word Embeddings (source)\n",
    "In the book example given above, our supervised task could be “identify whether or not a book was written by Leo Tolstoy” and the resulting embeddings would place books written by Tolstoy closer to each other. Figuring out how to create the supervised task to produce relevant representations is the toughest part of making embeddings.\n",
    "\n",
    "Implementation\n",
    "In the Wikipedia book project (complete notebook here), the supervised learning task is set as predicting whether a given link to a Wikipedia page appears in the article for a book. We feed in pairs of (book title, link) training examples with a mix of positive — true — and negative — false — pairs. This set-up is based on the assumption that books which link to similar Wikipedia pages are similar to one another. The resulting embeddings should therefore place alike books closer together in vector space.\n",
    "\n",
    "The network I used has two parallel embedding layers that map the book and wikilink to separate 50-dimensional vectors and a dot product layer that combines the embeddings into a single number for a prediction. The embeddings are the parameters, or weights, of the network that are adjusted during training to minimize the loss on the supervised task.\n",
    "\n",
    "In Keras code, this looks like the following (don’t worry if you don’t completely understand the code, just skip to the images):\n",
    "\n",
    "\n",
    "Although in a supervised machine learning task the goal is usually to train a model to make predictions on new data, in this embedding model, the predictions can be just a means to an end. What we want is the embedding weights, the representation of the books and links as continuous vectors.\n",
    "\n",
    "The embeddings by themselves are not that interesting: they are simply vectors of numbers:\n",
    "\n",
    "\n",
    "Example Embeddings from Book Recommendation Embedding Model\n",
    "However, the embeddings can be used for the 3 purposes listed previously, and for this project, we are primarily interested in recommending books based on the nearest neighbors. To compute similarity, we take a query book and find the dot product between its vector and those of all the other books. (If our embeddings are normalized, this dot product is the cosine distance between vectors that ranges from -1, most dissimilar, to +1, most similar. We could also use the Euclidean distance to measure similarity).\n",
    "\n",
    "This is the output of the book embedding model I built:\n",
    "\n",
    "Books closest to War and Peace.\n",
    "Book: War and Peace              Similarity: 1.0\n",
    "Book: Anna Karenina              Similarity: 0.79\n",
    "Book: The Master and Margarita   Similarity: 0.77\n",
    "Book: Doctor Zhivago (novel)     Similarity: 0.76\n",
    "Book: Dead Souls                 Similarity: 0.75\n",
    "(The cosine similarity between a vector and itself must be 1.0). After some dimensionality reduction (see below), we can make figures like the following:\n",
    "\n",
    "\n",
    "Embedding Books with Closest Neighbors\n",
    "We can clearly see the value of learning embeddings! We now have a 50-number representation of every single book on Wikipedia, with similar books closer to one another.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e7913b",
   "metadata": {},
   "source": [
    "# How to update a keras \n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', input_shape=(10,)))\n",
    "model.add(Dense(5, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "a = np.array(model.get_weights())         # save weights in a np.array of np.arrays\n",
    "model.set_weights(a + 1)                  # add 1 to all weights in the neural network\n",
    "b = np.array(model.get_weights())         # save weights a second time in a np.array of np.arrays\n",
    "print(b - a)                              # print changes in weights\n",
    "####################################################################\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import backend as k\n",
    "from keras import losses\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(8, kernel_initializer='uniform', activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "inputs = np.random.random((1, 8))\n",
    "outputs = model.predict(inputs)\n",
    "targets = np.random.random((1, 8))\n",
    "rmse = sqrt(mean_squared_error(targets, outputs))\n",
    "\n",
    "print(\"===BEFORE WALKING DOWN GRADIENT===\")\n",
    "print(\"outputs:\\n\", outputs)\n",
    "print(\"targets:\\n\", targets)\n",
    "print(\"RMSE:\", rmse)\n",
    "\n",
    "\n",
    "def descend(steps=40, learning_rate=100.0, learning_decay=0.95):\n",
    "    for s in range(steps):\n",
    "\n",
    "        # If your target changes, you need to update the loss\n",
    "        loss = losses.mean_squared_error(targets, model.output)\n",
    "\n",
    "        #  ===== Symbolic Gradient =====\n",
    "        # Tensorflow Tensor Object\n",
    "        gradients = k.gradients(loss, model.trainable_weights)\n",
    "\n",
    "        # ===== Numerical gradient =====\n",
    "        # Numpy ndarray Objcet\n",
    "        evaluated_gradients = sess.run(gradients, feed_dict={model.input: inputs})\n",
    "\n",
    "        # For every trainable layer in the network\n",
    "        for i in range(len(model.trainable_weights)):\n",
    "\n",
    "            layer = model.trainable_weights[i]  # Select the layer\n",
    "\n",
    "            # And modify it explicitly in TensorFlow\n",
    "            sess.run(tf.assign_sub(layer, learning_rate * evaluated_gradients[i]))\n",
    "\n",
    "        # decrease the learning rate\n",
    "        learning_rate *= learning_decay\n",
    "\n",
    "        outputs = model.predict(inputs)\n",
    "        rmse = sqrt(mean_squared_error(targets, outputs))\n",
    "\n",
    "        print(\"RMSE:\", rmse)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Begin TensorFlow\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    descend(steps=5)\n",
    "\n",
    "    final_outputs = model.predict(inputs)\n",
    "    final_rmse = sqrt(mean_squared_error(targets, final_outputs))\n",
    "\n",
    "    print(\"===AFTER STEPPING DOWN GRADIENT===\")\n",
    "    print(\"outputs:\\n\", final_outputs)\n",
    "    print(\"targets:\\n\", targets)\n",
    "    \n",
    "    \n",
    "# in LSTM\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(n_neurons, input_shape=(n_seq, n_features)))\n",
    "model.add(layers.Dense(n_pred_seq * n_features))\n",
    "model.add(layers.Reshape((n_pred_seq, n_features)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "And here is the way on which I’m updating the model:\n",
    "\n",
    "y_pred = model.predict_on_batch(x_batch)\n",
    "up_y = data_y[i,]\n",
    "a_score = sqrt(mean_squared_error(data_y[i,].flatten(), y_pred[0, :]))\n",
    "w = model.layers[0].get_weights() #Only get weights for LSTM layer\n",
    "for l in range(len(w)):\n",
    "    w[l] = w[l] - (w[l]*0.001*a_score) #0.001=learning rate\n",
    "model.layers[0].set_weights(w)\n",
    "model.fit(x_batch, up_y, epochs=1, verbose=1)\n",
    "model.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e75254",
   "metadata": {},
   "source": [
    "# Is it possible to stop machine learning training and continue it later?\n",
    "5\n",
    "\n",
    "\n",
    "In frameworks like PyTorch or Tensorflow it is pretty simple. You can save the weights of your model and then to restore those weights later all you have to do is make an instance of your model and load your weights.\n",
    "\n",
    "For PyTorch, you basically do this:\n",
    "\n",
    "torch.save(model.state_dict(), path_to_save_to)\n",
    "When you want to load saved weights:\n",
    "\n",
    "model = ModelClass()\n",
    "\n",
    "model.load_state_dict(torch.load(path_saved_to)\n",
    "You may want to save after every epoch or after every n epochs or only when your model's performance goes up etc.\n",
    "\n",
    "If you are not using any frameworks, it is possible even then as well. You can save your model weights in a Numpy array which you can then save to your Gdrive in several ways. When needed again, instantiate your model, instead of randomly initializing your parameters, set them to your loaded Numpy array.\n",
    "                      ____________________________________________________________________________________________\n",
    "                      # Keras: Starting, stopping, and resuming training\n",
    "                      Tutorial Overview\n",
    "This tutorial is divided into six parts; they are:\n",
    "\n",
    "Using Callbacks in Keras\n",
    "Evaluating a Validation Dataset\n",
    "Monitoring Model Performance\n",
    "Early Stopping in Keras\n",
    "Checkpointing in Keras\n",
    "Early Stopping Case Study\n",
    "\n",
    "Using Callbacks in Keras\n",
    "Callbacks provide a way to execute code and interact with the training model process automatically.\n",
    "\n",
    "Callbacks can be provided to the fit() function via the “callbacks” argument.\n",
    "\n",
    "First, callbacks must be instantiated.\n",
    "\n",
    "...\n",
    "cb = Callback(...)\n",
    "Then, one or more callbacks that you intend to use must be added to a Python list.\n",
    "\n",
    "...\n",
    "cb_list = [cb, ...]\n",
    "Finally, the list of callbacks is provided to the callback argument when fitting the model.\n",
    "\n",
    "...\n",
    "model.fit(..., callbacks=cb_list)\n",
    "\n",
    "Evaluating a Validation Dataset in Keras\n",
    "Early stopping requires that a validation dataset is evaluated during training.\n",
    "\n",
    "This can be achieved by specifying the validation dataset to the fit() function when training your model.\n",
    "\n",
    "There are two ways of doing this.\n",
    "\n",
    "The first involves you manually splitting your training data into a train and validation dataset and specifying the validation dataset to the fit() function via the validation_data argument. For example:\n",
    "\n",
    "...\n",
    "model.fit(train_X, train_y, validation_data=(val_x, val_y))\n",
    "Alternately, the fit() function can automatically split your training dataset into train and validation sets based on a percentage split specified via the validation_split argument.\n",
    "\n",
    "The validation_split is a value between 0 and 1 and defines the percentage amount of the training dataset to use for the validation dataset. For example:\n",
    "\n",
    "...\n",
    "model.fit(train_X, train_y, validation_split=0.3)\n",
    "In both cases, the model is not trained on the validation dataset. Instead, the model is evaluated on the validation dataset at the end of each training epoch.\n",
    "\n",
    "Want Better Results with Deep Learning?\n",
    "Take my free 7-day email crash course now (with sample code).\n",
    "\n",
    "Click to sign-up and also get a free PDF Ebook version of the course.\n",
    "\n",
    "Download Your FREE Mini-Course\n",
    "\n",
    "\n",
    "Monitoring Model Performance\n",
    "The loss function chosen to be optimized for your model is calculated at the end of each epoch.\n",
    "\n",
    "To callbacks, this is made available via the name “loss.”\n",
    "\n",
    "If a validation dataset is specified to the fit() function via the validation_data or validation_split arguments, then the loss on the validation dataset will be made available via the name “val_loss.”\n",
    "\n",
    "Additional metrics can be monitored during the training of the model.\n",
    "\n",
    "They can be specified when compiling the model via the “metrics” argument to the compile function. This argument takes a Python list of known metric functions, such as ‘mse‘ for mean squared error and ‘accuracy‘ for accuracy. For example:\n",
    "\n",
    "...\n",
    "model.compile(..., metrics=['accuracy'])\n",
    "If additional metrics are monitored during training, they are also available to the callbacks via the same name, such as ‘accuracy‘ for accuracy on the training dataset and ‘val_accuracy‘ for the accuracy on the validation dataset. Or, ‘mse‘ for mean squared error on the training dataset and ‘val_mse‘ on the validation dataset.\n",
    "\n",
    "\n",
    "Early Stopping in Keras\n",
    "Keras supports the early stopping of training via a callback called EarlyStopping.\n",
    "\n",
    "This callback allows you to specify the performance measure to monitor, the trigger, and once triggered, it will stop the training process.\n",
    "\n",
    "The EarlyStopping callback is configured when instantiated via arguments.\n",
    "\n",
    "The “monitor” allows you to specify the performance measure to monitor in order to end training. Recall from the previous section that the calculation of measures on the validation dataset will have the ‘val_‘ prefix, such as ‘val_loss‘ for the loss on the validation dataset.\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss')\n",
    "Based on the choice of performance measure, the “mode” argument will need to be specified as whether the objective of the chosen metric is to increase (maximize or ‘max‘) or to decrease (minimize or ‘min‘).\n",
    "\n",
    "For example, we would seek a minimum for validation loss and a minimum for validation mean squared error, whereas we would seek a maximum for validation accuracy.\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min')\n",
    "By default, mode is set to ‘auto‘ and knows that you want to minimize loss or maximize accuracy.\n",
    "\n",
    "That is all that is needed for the simplest form of early stopping. Training will stop when the chosen performance measure stops improving. To discover the training epoch on which training was stopped, the “verbose” argument can be set to 1. Once stopped, the callback will print the epoch number.\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "Often, the first sign of no further improvement may not be the best time to stop training. This is because the model may coast into a plateau of no improvement or even get slightly worse before getting much better.\n",
    "\n",
    "We can account for this by adding a delay to the trigger in terms of the number of epochs on which we would like to see no improvement. This can be done by setting the “patience” argument.\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "The exact amount of patience will vary between models and problems. Reviewing plots of your performance measure can be very useful to get an idea of how noisy the optimization process for your model on your data may be.\n",
    "\n",
    "By default, any change in the performance measure, no matter how fractional, will be considered an improvement. You may want to consider an improvement that is a specific increment, such as 1 unit for mean squared error or 1% for accuracy. This can be specified via the “min_delta” argument.\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', min_delta=1)\n",
    "Finally, it may be desirable to only stop training if performance stays above or below a given threshold or baseline. For example, if you have familiarity with the training of the model (e.g. learning curves) and know that once a validation loss of a given value is achieved that there is no point in continuing training. This can be specified by setting the “baseline” argument.\n",
    "\n",
    "This might be more useful when fine tuning a model, after the initial wild fluctuations in the performance measure seen in the early stages of training a new model are past.\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', baseline=0.4)\n",
    "\n",
    "Checkpointing in Keras\n",
    "The EarlyStopping callback will stop training once triggered, but the model at the end of training may not be the model with best performance on the validation dataset.\n",
    "\n",
    "An additional callback is required that will save the best model observed during training for later use. This is the ModelCheckpoint callback.\n",
    "\n",
    "The ModelCheckpoint callback is flexible in the way it can be used, but in this case we will use it only to save the best model observed during training as defined by a chosen performance measure on the validation dataset.\n",
    "\n",
    "Saving and loading models requires that HDF5 support has been installed on your workstation. For example, using the pip Python installer, this can be achieved as follows:\n",
    "\n",
    "sudo pip install h5py\n",
    "You can learn more from the h5py Installation documentation.\n",
    "\n",
    "The callback will save the model to file, which requires that a path and filename be specified via the first argument.\n",
    "\n",
    "mc = ModelCheckpoint('best_model.h5')\n",
    "The preferred loss function to be monitored can be specified via the monitor argument, in the same way as the EarlyStopping callback. For example, loss on the validation dataset (the default).\n",
    "\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_loss')\n",
    "Also, as with the EarlyStopping callback, we must specify the “mode” as either minimizing or maximizing the performance measure. Again, the default is ‘auto,’ which is aware of the standard performance measures.\n",
    "\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min')\n",
    "Finally, we are interested in only the very best model observed during training, rather than the best compared to the previous epoch, which might not be the best overall if training is noisy. This can be achieved by setting the “save_best_only” argument to True.\n",
    "\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "That is all that is needed to ensure the model with the best performance is saved when using early stopping, or in general.\n",
    "\n",
    "It may be interesting to know the value of the performance measure and at what epoch the model was saved. This can be printed by the callback by setting the “verbose” argument to “1“.\n",
    "\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1)\n",
    "The saved model can then be loaded and evaluated any time by calling the load_model() function.\n",
    "\n",
    "# load a saved model\n",
    "from keras.models import load_model\n",
    "saved_model = load_model('best_model.h5')\n",
    "Now that we know how to use the early stopping and model checkpoint APIs, let’s look at a worked example.\n",
    "\n",
    "\n",
    "Early Stopping Case Study\n",
    "In this section, we will demonstrate how to use early stopping to reduce overfitting of an MLP on a simple binary classification problem.\n",
    "\n",
    "This example provides a template for applying early stopping to your own neural network for classification and regression problems.\n",
    "\n",
    "Binary Classification Problem\n",
    "We will use a standard binary classification problem that defines two semi-circles of observations, one semi-circle for each class.\n",
    "\n",
    "Each observation has two input variables with the same scale and a class output value of either 0 or 1. This dataset is called the “moons” dataset because of the shape of the observations in each class when plotted.\n",
    "\n",
    "We can use the make_moons() function to generate observations from this problem. We will add noise to the data and seed the random number generator so that the same samples are generated each time the code is run.\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n",
    "We can plot the dataset where the two variables are taken as x and y coordinates on a graph and the class value is taken as the color of the observation.\n",
    "\n",
    "The complete example of generating the dataset and plotting it is listed below.\n",
    "\n",
    "# generate two moons dataset\n",
    "from sklearn.datasets import make_moons\n",
    "from matplotlib import pyplot\n",
    "from pandas import DataFrame\n",
    "# generate 2d classification dataset\n",
    "X, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n",
    "# scatter plot, dots colored by class value\n",
    "df = DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n",
    "colors = {0:'red', 1:'blue'}\n",
    "fig, ax = pyplot.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "pyplot.show()\n",
    "Running the example creates a scatter plot showing the semi-circle or moon shape of the observations in each class. We can see the noise in the dispersal of the points making the moons less obvious.\n",
    "\n",
    "Scatter Plot of Moons Dataset With Color Showing the Class Value of Each Sample\n",
    "Scatter Plot of Moons Dataset With Color Showing the Class Value of Each Sample\n",
    "\n",
    "This is a good test problem because the classes cannot be separated by a line, e.g. are not linearly separable, requiring a nonlinear method such as a neural network to address.\n",
    "\n",
    "We have only generated 100 samples, which is small for a neural network, providing the opportunity to overfit the training dataset and have higher error on the test dataset: a good case for using regularization. Further, the samples have noise, giving the model an opportunity to learn aspects of the samples that don’t generalize.\n",
    "\n",
    "\n",
    "Overfit Multilayer Perceptron\n",
    "We can develop an MLP model to address this binary classification problem.\n",
    "\n",
    "The model will have one hidden layer with more nodes than may be required to solve this problem, providing an opportunity to overfit. We will also train the model for longer than is required to ensure the model overfits.\n",
    "\n",
    "Before we define the model, we will split the dataset into train and test sets, using 30 examples to train the model and 70 to evaluate the fit model’s performance.\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n",
    "# split into train and test\n",
    "n_train = 30\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "Next, we can define the model.\n",
    "\n",
    "The hidden layer uses 500 nodes and the rectified linear activation function. A sigmoid activation function is used in the output layer in order to predict class values of 0 or 1. The model is optimized using the binary cross entropy loss function, suitable for binary classification problems and the efficient Adam version of gradient descent.\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=2, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "The defined model is then fit on the training data for 4,000 epochs and the default batch size of 32.\n",
    "\n",
    "We will also use the test dataset as a validation dataset. This is just a simplification for this example. In practice, you would split the training set into train and validation and also hold back a test set for final model evaluation.\n",
    "\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=4000, verbose=0)\n",
    "We can evaluate the performance of the model on the test dataset and report the result.\n",
    "\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "Finally, we will plot the loss of the model on both the train and test set each epoch.\n",
    "\n",
    "If the model does indeed overfit the training dataset, we would expect the line plot of loss (and accuracy) on the training set to continue to increase and the test set to rise and then fall again as the model learns statistical noise in the training dataset.\n",
    "\n",
    "# plot training history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "We can tie all of these pieces together; the complete example is listed below.\n",
    "\n",
    "# mlp overfit on the moons dataset\n",
    "from sklearn.datasets import make_moons\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from matplotlib import pyplot\n",
    "# generate 2d classification dataset\n",
    "X, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n",
    "# split into train and test\n",
    "n_train = 30\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=2, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=4000, verbose=0)\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "# plot training history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "Running the example reports the model performance on the train and test datasets.\n",
    "\n",
    "We can see that the model has better performance on the training dataset than the test dataset, one possible sign of overfitting.\n",
    "\n",
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "Because the model is severely overfit, we generally would not expect much, if any, variance in the accuracy across repeated runs of the model on the same dataset.\n",
    "\n",
    "Train: 1.000, Test: 0.914\n",
    "A figure is created showing line plots of the model loss on the train and test sets.\n",
    "\n",
    "We can see that expected shape of an overfit model where test accuracy increases to a point and then begins to decrease again.\n",
    "\n",
    "Reviewing the figure, we can also see flat spots in the ups and downs in the validation loss. Any early stopping will have to account for these behaviors. We would also expect that a good time to stop training might be around epoch 800.\n",
    "\n",
    "Line Plots of Loss on Train and Test Datasets While Training Showing an Overfit Model\n",
    "Line Plots of Loss on Train and Test Datasets While Training Showing an Overfit Model\n",
    "\n",
    "\n",
    "Overfit MLP With Early Stopping\n",
    "We can update the example and add very simple early stopping.\n",
    "\n",
    "As soon as the loss of the model begins to increase on the test dataset, we will stop training.\n",
    "\n",
    "First, we can define the early stopping callback.\n",
    "\n",
    "# simple early stopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "We can then update the call to the fit() function and specify a list of callbacks via the “callback” argument.\n",
    "\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=4000, verbose=0, callbacks=[es])\n",
    "The complete example with the addition of simple early stopping is listed below.\n",
    "\n",
    "# mlp overfit on the moons dataset with simple early stopping\n",
    "from sklearn.datasets import make_moons\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from matplotlib import pyplot\n",
    "# generate 2d classification dataset\n",
    "X, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n",
    "# split into train and test\n",
    "n_train = 30\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=2, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# simple early stopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=4000, verbose=0, callbacks=[es])\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "# plot training history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "Running the example reports the model performance on the train and test datasets.\n",
    "\n",
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "We can also see that the callback stopped training at epoch 200. This is too early as we would expect an early stop to be around epoch 800. This is also highlighted by the classification accuracy on both the train and test sets, which is worse than no early stopping.\n",
    "\n",
    "Epoch 00219: early stopping\n",
    "Train: 0.967, Test: 0.814\n",
    "Reviewing the line plot of train and test loss, we can indeed see that training was stopped at the point when validation loss began to plateau for the first time.\n",
    "\n",
    "Line Plot of Train and Test Loss During Training With Simple Early Stopping\n",
    "Line Plot of Train and Test Loss During Training With Simple Early Stopping\n",
    "\n",
    "We can improve the trigger for early stopping by waiting a while before stopping.\n",
    "\n",
    "This can be achieved by setting the “patience” argument.\n",
    "\n",
    "In this case, we will wait 200 epochs before training is stopped. Specifically, this means that we will allow training to continue for up to an additional 200 epochs after the point that validation loss started to degrade, giving the training process an opportunity to get across flat spots or find some additional improvement.\n",
    "\n",
    "# patient early stopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
    "The complete example with this change is listed below.\n",
    "\n",
    "# mlp overfit on the moons dataset with patient early stopping\n",
    "from sklearn.datasets import make_moons\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from matplotlib import pyplot\n",
    "# generate 2d classification dataset\n",
    "X, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n",
    "# split into train and test\n",
    "n_train = 30\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=2, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# patient early stopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=4000, verbose=0, callbacks=[es])\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "# plot training history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "Running the example, we can see that training was stopped much later, in this case after epoch 1,000.\n",
    "\n",
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "We can also see that the performance on the test dataset is better than not using any early stopping.\n",
    "\n",
    "Epoch 01033: early stopping\n",
    "Train: 1.000, Test: 0.943\n",
    "Reviewing the line plot of loss during training, we can see that the patience allowed the training to progress past some small flat and bad spots.\n",
    "\n",
    "Line Plot of Train and Test Loss During Training With Patient Early Stopping\n",
    "Line Plot of Train and Test Loss During Training With Patient Early Stopping\n",
    "\n",
    "We can also see that test loss started to increase again in the last approximately 100 epochs.\n",
    "\n",
    "This means that although the performance of the model has improved, we may not have the best performing or most stable model at the end of training. We can address this by using a ModelChecckpoint callback.\n",
    "\n",
    "In this case, we are interested in saving the model with the best accuracy on the test dataset. We could also seek the model with the best loss on the test dataset, but this may or may not correspond to the model with the best accuracy.\n",
    "\n",
    "This highlights an important concept in model selection. The notion of the “best” model during training may conflict when evaluated using different performance measures. Try to choose models based on the metric by which they will be evaluated and presented in the domain. In a balanced binary classification problem, this will most likely be classification accuracy. Therefore, we will use accuracy on the validation in the ModelCheckpoint callback to save the best model observed during training.\n",
    "\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "During training, the entire model will be saved to the file “best_model.h5” only when accuracy on the validation dataset improves overall across the entire training process. A verbose output will also inform us as to the epoch and accuracy value each time the model is saved to the same file (e.g. overwritten).\n",
    "\n",
    "This new additional callback can be added to the list of callbacks when calling the fit() function.\n",
    "\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=4000, verbose=0, callbacks=[es, mc])\n",
    "We are no longer interested in the line plot of loss during training; it will be much the same as the previous run.\n",
    "\n",
    "Instead, we want to load the saved model from file and evaluate its performance on the test dataset.\n",
    "\n",
    "# load the saved model\n",
    "saved_model = load_model('best_model.h5')\n",
    "# evaluate the model\n",
    "_, train_acc = saved_model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = saved_model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "The complete example with these changes is listed below.\n",
    "\n",
    "# mlp overfit on the moons dataset with patient early stopping and model checkpointing\n",
    "from sklearn.datasets import make_moons\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from matplotlib import pyplot\n",
    "from keras.models import load_model\n",
    "# generate 2d classification dataset\n",
    "X, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n",
    "# split into train and test\n",
    "n_train = 30\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=2, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# simple early stopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=4000, verbose=0, callbacks=[es, mc])\n",
    "# load the saved model\n",
    "saved_model = load_model('best_model.h5')\n",
    "# evaluate the model\n",
    "_, train_acc = saved_model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = saved_model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "Running the example, we can see the verbose output from the ModelCheckpoint callback for both when a new best model is saved and from when no improvement was observed.\n",
    "\n",
    "We can see that the best model was observed at epoch 879 during this run.\n",
    "\n",
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "Again, we can see that early stopping continued patiently until after epoch 1,000. Note that epoch 880 + a patience of 200 is not epoch 1044. Recall that early stopping is monitoring loss on the validation dataset and that the model checkpoint is saving models based on accuracy. As such, the patience of early stopping started at an epoch other than 880.\n",
    "\n",
    "...\n",
    "Epoch 00878: val_acc did not improve from 0.92857\n",
    "Epoch 00879: val_acc improved from 0.92857 to 0.94286, saving model to best_model.h5\n",
    "Epoch 00880: val_acc did not improve from 0.94286\n",
    "...\n",
    "Epoch 01042: val_acc did not improve from 0.94286\n",
    "Epoch 01043: val_acc did not improve from 0.94286\n",
    "Epoch 01044: val_acc did not improve from 0.94286\n",
    "Epoch 01044: early stopping\n",
    "Train: 1.000, Test: 0.943\n",
    "In this case, we don’t see any further improvement in model accuracy on the test dataset. Nevertheless, we have followed a good practice.\n",
    "\n",
    "Why not monitor validation accuracy for early stopping?\n",
    "\n",
    "This is a good question. The main reason is that accuracy is a coarse measure of model performance during training and that loss provides more nuance when using early stopping with classification problems. The same measure may be used for early stopping and model checkpointing in the case of regression, such as mean squared error\n",
    "\n",
    "LInk foe\\r info:.\n",
    "https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecf49f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
