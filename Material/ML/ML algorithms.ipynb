{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72dc414c",
   "metadata": {},
   "source": [
    "# k- Nearest Neighbors(KNN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e18e882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as Pl\n",
    "from sklearn import neighbors, datasets\n",
    "# import some data to play with\n",
    "iris= datasets.load_iris()\n",
    "X = iris.data[:,:2] # we only take the first two features\n",
    "Y = iris.target\n",
    "h= .02 # step size in the mesh\n",
    "knn=neighbors. KNeighborsClassifier()\n",
    "#we create an instance of neighbors classifier and fit data \n",
    "knn.fit(x,y)\n",
    "# Plot the decision boundary , we will assign colors\n",
    "# point in the mesh [x_min, m_ma]x[y_min,y_max].\n",
    "×_min, x_max= X[:,0].min()\n",
    "y_min, y_max = X[:,1] .min()\n",
    "xx, yy = np.meshgrid(np.arange(x min, × max, h), np.arange(y_min, y_max,h)\n",
    "z= knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "# Put the result into a color pLot\n",
    "z = Z.reshape (xx.shape)\n",
    "pl.figure(1, figsizes (4, 3))\n",
    "pl.set_стар(pl.cm.Paired)\n",
    "pl.pcolormesh(xx, yy, Z)\n",
    "# Plot also the traning points\n",
    "pl.scatter (X[:,0], X[:,1], c='red' )\n",
    "pl.xlabel('Sepal length')\n",
    "pl.ylabel('Sepal width')\n",
    "pl.xlim(xx.min(), xx.max())\n",
    "pl.ylim(yy.min(), yy.max())\n",
    "pl.xticks (().\n",
    "pl.yticks(())\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04593b6",
   "metadata": {},
   "source": [
    "# Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35294e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed (0)\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "print (df. head())\n",
    "df ['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n",
    "print (df. head() )\n",
    "df['is_train'] = np.random.uniform(0, 1, len(df)) <= .75\n",
    "print(df.head())\n",
    "train, test = df [df['is_train']==True], df[df['is_train']==false]\n",
    "features = df. columns [:4]\n",
    "y = pd.factorize(train['species'])[0]\n",
    "clf = RandomForestClassifier (n_jobs=2, random_state=0)\n",
    "clf.fit(train[features], y)\n",
    "print ( clf.predict(test[features]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff8218c",
   "metadata": {},
   "source": [
    "# GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c950b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[: 2000], y[2000:]\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "print ( clf.score(X_test, y_test) )\n",
    "\n",
    "###########################################################################################################################\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "X, y = make_friedman1 (n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, _test = y[:200], y[200:]\n",
    "est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0, loss='Is').fit(X_train, _train)\n",
    "mean_squared_error(y_test, est.predict (X_test))\n",
    "print ( \"Score :\", est.score (X_test, y_test) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c94589c",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb0fe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97459d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print('Load data...')\n",
    "df train = pd.read_csv(\" ../regression/regression. train\", header=None, sep='\\t')\n",
    "df_test = pd.read_csv(' ../regression/regression.test', header=None, sep='\\t')\n",
    "y_train = df_train[0]. values\n",
    "y_test = df_test[0].values\n",
    "X_train = df_train.drop(0, axis=1). values\n",
    "X_test = df_test.drop(0, axis=1). values\n",
    "\n",
    "lgb_train = Igb.Dataset(X_train, y_train)\n",
    "lgb_eval = Igb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': {'12','auc'},\n",
    "    'num leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "print('Start training...')\n",
    "# train\n",
    "gbm = Igb. train(params, Igb_train, num_boost_round=20, valid_sets=lgb_eval,early_stopping_rounds=5)\n",
    "print('Save model...')\n",
    "gbm. save_model('model.txt')\n",
    "print('Start predicting...')\n",
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "print('The rmse of prediction is:' , mean_squared_error (y_test, y_pred) ** 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ba1a42",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4966b1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First XGBoost model for Pima Indians dataset\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy score\n",
    "# Load data\n",
    "dataset = loadtxt ('pima-indians-diabetes.csv', delimiter=\",\")\n",
    "# split data into X and y\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# split data into train and test sets\n",
    "seed = 7\n",
    "test_size = 0.33\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "model. fit(X_train, y_train)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round (value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score (y_test, predictions)\n",
    "print(\"Accuracy: %.2£%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e5c475",
   "metadata": {},
   "source": [
    "# Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6810f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e147cbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "iris = datasets.load_iris()\n",
    "iris_X = iris.data\n",
    "iris_y = iris.target\n",
    "np. random. seed (0)\n",
    "indices = np.random.permutation(len (iris_X))\n",
    "iris_X_train = iris_X[indices[:-10]]\n",
    "iris_y_train = iris_y[indices [:-10]]\n",
    "iris_X_test = iris_X[indices[-10:]]\n",
    "iris_y_test = iris_y[indices[-10:]]\n",
    "\n",
    "cls = catboost.CatBoostClassifier(loss_function='MultiClass')\n",
    "\n",
    "cls.fit(iris_X_train, iris_y_train)\n",
    "preds = cls.predict(iris_X_test)\n",
    "print (preds)\n",
    "# Save model to catboost format\n",
    "cls.save_model(\"iris.mImodel\", format=\"corem]\"\n",
    "    export_parameters={'prediction_type': 'probability'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a9379f",
   "metadata": {},
   "source": [
    "# simple neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cc3401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# sigmoid function\n",
    "def nonlin(x, deriv=False):\n",
    "    if(deriv==True):\n",
    "        return ×*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "# input dataset\n",
    "X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n",
    "# output dataset\n",
    "y = np.array ([ [0, 0,1,1]]) . T\n",
    "# seed random numbers Fo make calculation\n",
    "# deterministic (just a good practice)\n",
    "np.random. seed(1)\n",
    "# initialize weights randomly with mean 0\n",
    "syn0 = 2*p. random. random((3,1)) - 1\n",
    "for iter in range (10000):\n",
    "    l0 = X\n",
    "    l1 = nonlin(np. dot (10, syn))\n",
    "    l1_error = y - l1\n",
    "    # multiply how much we missed by the\n",
    "    # slope of the sigmoid at the values in l1\n",
    "    l1_delta = l1_error * nonlin(l1, True)\n",
    "    syn0 += np.dot (l0.T,11_delta)\n",
    "print (\"Output After Training: \\n\" ,l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeaed27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import exp, array, random, dot\n",
    "training_set_inputs = array([[0, 0, 1],[1, 1, 1],[1, 0, 1],[0, 1, 1]])\n",
    "training_set_outputs = array([[0, 1, 1, 0]]).T\n",
    "random. seed (1)\n",
    "synaptic_weights = random.random((3, 1))\n",
    "for iteration in range(10000):\n",
    "    output = 1 / (1 + exp(-(dot (training_set_inputs, synaptic_weights))))\n",
    "    synaptic_wgights += dot(training_set_inputs.T, (training_set_outputs - output)\n",
    "    * output * (1 - output))\n",
    "\n",
    "print (\"predict [1, 0, 0] : \", 1 / (1 + exp(- (dot (array ([1, 0, 0]), synaptic_weights)))))\n",
    "print (\"predict [0, 0, 0] : \", 1 / (1 + exp(- (dot (array ([0, 0, 0]), synaptic_weights)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be264d24",
   "metadata": {},
   "source": [
    "# multi-lavered neural network in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c0ffc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import exp, array, random, dot\n",
    "class NeuronLayer():\n",
    "    def __init__(self, number_of_neurons, number_of_inputs_per_neuron):\n",
    "        self.synaptic_weights = 2 * random.random (number_of_inputs_per_neuron, number_of_neurons)\n",
    "class NeuralNetwork () :\n",
    "    def init_(self, layer1, layer2):\n",
    "        self.layer1 = layer1\n",
    "        self.layer2 = layer2\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + exp(-x))\n",
    "    def _sigmoid_derivative (self, x):\n",
    "        return x * (1 - x)\n",
    "    def train(self, training_set_inputs, training_set_outputs, number_of_training_iterations\n",
    "        for iteration in range(number_of_training_iterations):\n",
    "        output_from_layer_1, output_from_layer_2 = self.think(training_set_inputs)\n",
    "        layer2_error = training_set_outputs - output_from_layer_2\n",
    "        layer2_delta = layer2_error * self.__sigmoid_derivative(output_from_layer_2)\n",
    "        layer1_error = layer2_delta.dot (self.layer2.synaptic_weights.T)\n",
    "        layer1_delta = layer1_error * self.__sigmoid_derivative(output_from_layer_1)\n",
    "        layer1_adjustment = training_set_inputs.T.dot (layer1_delta)\n",
    "        layer2_adjustment = output_from_layer_1.T.dot(layer2_delta)\n",
    "        self.layer1.synaptic_weights += layer1_adjustment \n",
    "        self.layer2.synaptic_weights += layer2_adjustment\n",
    "    def think(self, inputs):\n",
    "        output_from_layer1 = self._sigmoid(dot (inputs, self.layer1.synaptic_weights))\n",
    "        output_from_layer2 = self._sigmoid(dot (output_from_layer1, self.layer2.synaptic_weights))\n",
    "        return output_from_layer1, output_from_layer2\n",
    "    # The neural network prints its weights\n",
    "    def print_weights(self):\n",
    "        print (\"Layer 1 (4 neurons, each with 3 inputs): \")\n",
    "        print (self.layer1.synaptic_weights)\n",
    "        print (\"Layer 2 (1 neuron, with 4 inputs):\")\n",
    "        print (self.layer2.synaptic_weights)\n",
    "random. seed(1)\n",
    "layer1 = NeuronLayer (4, 3)\n",
    "layer2 = NeuronLayer (1, 4)\n",
    "neural_network = NeuralNetwork(layer1, layer2)\n",
    "print (\"Stage 1- Random starting synaptic weights: \")\n",
    "neural_network.print_weights()\n",
    "training_set_inputs = array ([[0, 0, 1], [0, 1, 1], [1, 0, 1], [0, 1, 0], [1, 0, 0], [1, 1, 1\n",
    "training_set_outputs = array([[0, 1, 1, 1, 1, 0, 0]]). T\n",
    "neural_network.train(training_set_inputs, training_set_outputs, 60000)\n",
    "\n",
    "print (\"Stage 2- New synaptic weights after training: \")\n",
    "neural_network.print_weights()\n",
    "\n",
    "# Test the neural network with a new situation.\n",
    "print (\"Stage 3- Considering a new situation [1, 1, 0] -> ?: \")\n",
    "hidden state, output neural_network. think(array([1, 1, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5009307e",
   "metadata": {},
   "source": [
    "# Architecture - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facf178d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "#Defining the model The Sequential model is a linear stack of layers.\n",
    "model = Sequential ()\n",
    "model.add (Dense (10, input_shape=(4,),activation='tanh'))\n",
    "model. add (Dense (8, activation='tanh'))\n",
    "model. add (Dense (6, activation='tanh'))\n",
    "model. add (Dense (3, activation='softmax'))\n",
    "\n",
    "model. compile(SGD(lr=0.1), loss='categorical _crossentropy',metrics=['accuracy'])\n",
    "# Stochastic gadient descent optimizer. Others : Adam,..\n",
    "model.summary ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682d423d",
   "metadata": {},
   "source": [
    "# notes:   \n",
    "the following snippets are strictly equivalent:\n",
    "\n",
    "model = sequential() model.add(Dense (32, input_shape= (784,)))\n",
    "\n",
    "model = Sequential() model.add(Dense (32, input_dim=784))\n",
    "___________________________________________________________\n",
    "You can also pass an element-wise\n",
    "TensorFlow/Theano/CNTK function as an activation \n",
    "\n",
    "from keras import backend as K \n",
    "\n",
    "model.add(Dense (64, activation=K.tanh))\n",
    "_______________________________________________________________\n",
    "Available activations : softmax, elu, selu, softplus, softsign, tanh, relu, sigmoid, linear, hard_sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a20c0f6",
   "metadata": {},
   "source": [
    "# Plotting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7d15c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Note: You need to install two packages: pydot and pygraphviz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e350544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils.vis_utils import plot_model\n",
    "model = Sequential ()\n",
    "model.add(Dense (2, input_dim=1, activation='relu'))\n",
    "model.add(Dense (1, activation='sigmoid'))\n",
    "plot_model (model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8e1be9",
   "metadata": {},
   "source": [
    "# XOR with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedc467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Import libraries and modules\n",
    "load data\n",
    "process data\n",
    "Define model architecture.\n",
    "Compile model\n",
    "Fit model on training data\n",
    "Evaluate model on test data \n",
    "Predict values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc64cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "\n",
    "X = np.array ([[0, 0], [0,1], [1,0], [1,1]])\n",
    "y = np.array ([ [0], [1], [1], [0]])\n",
    "\n",
    "model = Sequential ()\n",
    "model. add(Dense (8, input_dim=2))\n",
    "model.add (Activation('tanh'))\n",
    "model. add (Dense (1))\n",
    "model. add(Activation('sigmoid'))\n",
    "\n",
    "# can write by this way: model = Sequential([Dense (8, input_dim=2),Activation('tanh'), Dense (1),Activation('sigmoid')])\n",
    "\n",
    "sgd = SGD(1r=0.1)\n",
    "model. compile (loss='binary_crossentropy', optimizer=sgd)\n",
    "model. fit (X, y, batch_size=1, nb_epoch=1000)\n",
    "print (model.predict_proba(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129871a0",
   "metadata": {},
   "source": [
    "# Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce4790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a multi-class classification problem\n",
    "model.compile (optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# For a binary classification problem\n",
    "model.compile (optimizer='rmsprop', loss='binary_crossentropy',metrics=['accuracy'])\n",
    "# For a mean squared error regression problem\n",
    "model.compile (optimizer='rmsprop', loss='mse\")\n",
    "# For custom metrics\n",
    "import keras.backend as K\n",
    "def mean_pred (y_true, y_pred):\n",
    "    return K.mean(y_pred)\n",
    "model.compile(optimizer='rmsprop' loss='binary crossentropy', metrics=['accuracy', mean_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc7bb56",
   "metadata": {},
   "source": [
    "# IRIS dataset in 2 ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1df2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn. linear_model import LogisticRegressionCV\n",
    "iris = sns. load dataset(\"iris\")\n",
    "X = iris.values[:, :4]\n",
    "y = iris.values[:,4]\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, train_size=0.5,test_size=0.5, random_state=0)\n",
    "lr = LogisticRegressionCV()\n",
    "lr.fit(train _X, train_y)\n",
    "print(\"Accuracy = {:.2f}\".format(jr.score(test_X, test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e1f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "def one_hot_encode_object_array(arr):\n",
    "    '''One hot encode a numpy array of objects (e.g. strings)'''\n",
    "    uniques, ids = np.unique(arr, return_inverse=True)\n",
    "    return np_utils.to_categorical(ids, len(uniques))\n",
    "iris = sns. load_dataset (\"iris\")\n",
    "X = iris.values[:, :4]\n",
    "y = iris.values[:,4]\n",
    "train_X, test_X, train _y, test _y = train_test_split(X, y, train_size=0.5,test_size=0.5, random_state=0)\n",
    "train_y_ohe = one_hot_encode_object_array(train_y)\n",
    "test_y_ohe = one_hot_encode_object_array(test_y)\n",
    "model = Sequential ()\n",
    "model. add(Dense (16, input_shape=(4,)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense (3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical crossentropy', metrics=[\"accuracy\"])\n",
    "model.fit(train_X, train_y_ohe, epochs=100, batch_size=1, verbose=0);\n",
    "loss, accuracy = model.evaluate (test_X, test _y_ohe, verbose =0)\n",
    "print(\"Accuracy = {:.2f}\".format (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ea63f7",
   "metadata": {},
   "source": [
    "# Convolutional Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076516ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "# Initialising the CNN\n",
    "classifier = Sequential ()\n",
    "# Step 1 Convolution\n",
    "classifier.add(Conv2D (32, (3, 3), input_shape = (64, 64, 3),activation = 'relu'))\n",
    "# Step 2  Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D (32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D (pool_size = (2, 2)))\n",
    "# Step 3 Flattening\n",
    "classifier.add(Flatten ())\n",
    "# Step 4 Full connection\n",
    "classifier.add(Dense (units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Part 2 Fitting the CNN to the images\n",
    "from keras.preprocessing. image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator (rescale = 1./255,shear_range = 0.2,zoom_range = 0.2,horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator (rescale = 1./255)\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set',target_size = (64, 64), batch_size = 32,class_mode = 'binary')\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',target_size = (64, 64),batch_size = 32,class_mode = 'binary')\n",
    "classifier.fit_generator (training_set, steps_per_epoch = 8000, epochs = 25, validation_data = test_set, validation_steps = 2000)\n",
    "# Part 3 - Making new predictions\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image. load_img('dataset/single_prediction/cat_or_dog_1.jpg',target_size = (64, 64))\n",
    "test_image = image.img_to_array (test_image)\n",
    "test_image = np.expand_dims (test_image, axis = 0)\n",
    "result = classifier.predict (test_ image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'\n",
    "print (prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b071b08",
   "metadata": {},
   "source": [
    "# Refrences\n",
    "\n",
    "https://www.pythoncentral.io/introductory-tutorial-python-sqlalchemy/\n",
    "\n",
    "http://scikit-learn.org/stable/modules/ensemble.html\n",
    "\n",
    "https://chrisalbon.com/machine_learning/trees_and_forests/random_forest_classifier_example/\n",
    "\n",
    "http://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/auto_examples/tutorial/plot_knn_iris.html\n",
    "\n",
    "https://www.pythoncentral.io/introductory-tutorial-python-sqlalchemy/\n",
    "\n",
    "https://becominghuman.ai/building-an-image-classifier-using-deep-learning-in-python-totally-from-a-beginners-perspective-be8dbaf22dd8\n",
    "\n",
    "https://iamtrask.github.io/2015/07/12/basic-python-network/\n",
    "\n",
    "https://medium.com/technology-invention-and-more/how-to-build-a-simple-neural-network-in-9-lines-of-python-code-cc8f23647ca1\n",
    "\n",
    "https://medium.com/technology-invention-and-more/how-to-build-a-multi-layered-neural-network-in-python-53ec3d1d326a\n",
    "\n",
    "https://demo.bokehplots.com/\n",
    "\n",
    "https://machinelearningmastery.com/visualize-deep-learning-neural-network-model-keras/\n",
    "\n",
    "https://gist.github.com/stewartpark/187895beb89f0a1b3a54\n",
    "\n",
    "https://keras.io/getting-started/sequential-model-guide/\n",
    "\n",
    "https://demo.bokehplots.com/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82ec7b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
