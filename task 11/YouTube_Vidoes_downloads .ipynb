{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "428dc516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytube in c:\\users\\omnia hosny\\anaconda3\\lib\\site-packages (12.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d8aacd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/nficano/pytube\n",
      "  Cloning https://github.com/nficano/pytube to c:\\users\\omnia hosny\\appdata\\local\\temp\\pip-req-build-h53p2zac\n",
      "  Resolved https://github.com/nficano/pytube to commit 84faec34c8a66f502ac635a5610445dbff160654\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/nficano/pytube 'C:\\Users\\Omnia Hosny\\AppData\\Local\\Temp\\pip-req-build-h53p2zac'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/nficano/pytube\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7d24f444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting newspaper3k\n",
      "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
      "     ------------------------------------ 211.1/211.1 kB 513.8 kB/s eta 0:00:00\n",
      "Requirement already satisfied: lxml>=3.6.0 in c:\\users\\omnia hosny\\anaconda3\\lib\\site-packages (from newspaper3k) (4.6.3)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\omnia hosny\\anaconda3\\lib\\site-packages (from newspaper3k) (4.9.3)\n",
      "Collecting cssselect>=0.9.2\n",
      "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting jieba3k>=0.35.1\n",
      "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
      "     ---------------------------------------- 7.4/7.4 MB 685.6 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\users\\omnia hosny\\anaconda3\\lib\\site-packages (from newspaper3k) (2.25.1)\n",
      "Collecting tldextract>=2.0.1\n",
      "  Downloading tldextract-3.3.1-py3-none-any.whl (93 kB)\n",
      "     -------------------------------------- 93.6/93.6 kB 443.3 kB/s eta 0:00:00\n",
      "Collecting feedfinder2>=0.0.4\n",
      "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: Pillow>=3.3.0 in c:\\users\\omnia hosny\\anaconda3\\lib\\site-packages (from newspaper3k) (8.2.0)\n",
      "Requirement already satisfied: nltk>=3.2.1 in c:\\users\\omnia hosny\\anaconda3\\lib\\site-packages (from newspaper3k) (3.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\omnia hosny\\anaconda3\\lib\\site-packages (from newspaper3k) (2.8.1)\n",
      "Collecting tinysegmenter==0.3\n",
      "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: PyYAML>=3.11 in c:\\users\\omnia hosny\\anaconda3\\lib\\site-packages (from newspaper3k) (5.4.1)\n",
      "Collecting feedparser>=5.2.1\n",
      "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
      "     -------------------------------------- 81.1/81.1 kB 454.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\omnia hosny\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.2.1)\n",
      "Requirement already satisfied: six in c:\\users\\omnia hosny\\anaconda3\\lib\\site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.15.0)\n",
      "Collecting sgmllib3k\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: tqdm in c:\\users\\omnia hosny\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (4.59.0)\n",
      "Requirement already satisfied: regex in c:\\users\\omnia hosny\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (2021.4.4)\n",
      "Requirement already satisfied: click in c:\\users\\omnia hosny\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\omnia hosny\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (1.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\omnia hosny\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\omnia hosny\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\omnia hosny\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\omnia hosny\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (4.0.0)\n",
      "Collecting requests-file>=1.4\n",
      "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\omnia hosny\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (3.4.0)\n",
      "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
      "  Building wheel for tinysegmenter (setup.py): started\n",
      "  Building wheel for tinysegmenter (setup.py): finished with status 'done'\n",
      "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13551 sha256=d68e51a24decee998b193d50f6dae20db1f7d226bb6b688e71224b959af61097\n",
      "  Stored in directory: c:\\users\\omnia hosny\\appdata\\local\\pip\\cache\\wheels\\99\\74\\83\\8fac1c8d9c648cfabebbbffe97a889f6624817f3aa0bbe6c09\n",
      "  Building wheel for feedfinder2 (setup.py): started\n",
      "  Building wheel for feedfinder2 (setup.py): finished with status 'done'\n",
      "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3354 sha256=b7c7f4313f9ee02879a88be44c1aa3e30da506d105fc956f18c328a47e7de061\n",
      "  Stored in directory: c:\\users\\omnia hosny\\appdata\\local\\pip\\cache\\wheels\\b6\\09\\68\\a9f15498ac02c23dde29f18745bc6a6f574ba4ab41861a3575\n",
      "  Building wheel for jieba3k (setup.py): started\n",
      "  Building wheel for jieba3k (setup.py): finished with status 'done'\n",
      "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398404 sha256=f07a1096b4cd2cf855a308e60f7cbf9a2221ee4b8845dc114b8831465d5cfdbf\n",
      "  Stored in directory: c:\\users\\omnia hosny\\appdata\\local\\pip\\cache\\wheels\\1f\\7e\\0c\\54f3b0f5164278677899f2db08f2b07943ce2d024a3c862afb\n",
      "  Building wheel for sgmllib3k (setup.py): started\n",
      "  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=1e14a81cd4d51a710784033619e8f6949f62b8d05672dc01974d462a6187b232\n",
      "  Stored in directory: c:\\users\\omnia hosny\\appdata\\local\\pip\\cache\\wheels\\83\\63\\2f\\117884c3b19d46b64d3d61690333aa80c88dc14050e269c546\n",
      "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
      "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
      "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-6.0.10 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdd6e567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "from pytube import Playlist\n",
    "from pytube import Channel\n",
    "from newspaper import Article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb2236d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.youtube.com/watch?v=XoSS-EqhOUA'\n",
    "yt = YouTube(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4f37e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[30 SECONDS] SHORT ESP NMH Video 1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ab5bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "yt.streams.get_lowest_resolution().download('Desktop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dd9699a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.youtube.com/channel/UCpNnv_kL4Jk8YG_VflnZpmg'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt.channel_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3204317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PAHO TV'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt.author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1db28a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2020, 4, 29, 0, 0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt.publish_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0080c065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://i.ytimg.com/vi/XoSS-EqhOUA/sddefault.jpg'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt.thumbnail_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2878f2be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a.es': <Caption lang=\"Spanish (auto-generated)\" code=\"a.es\">}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt.captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d3ed91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "483"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt.views"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e299f1",
   "metadata": {},
   "source": [
    "## playlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c8a3b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.youtube.com/playlist?list=PLcitu3ZrbYgTsOsmMLUhBeBkKeSCChfzi'\n",
    "p = Playlist(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e989ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Les Noms d' Allah\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34062c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b4695ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.youtube.com/playlist?list=PLcitu3ZrbYgTsOsmMLUhBeBkKeSCChfzi'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.playlist_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95d97e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bissan ElBaz'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.owner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5a8242d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.youtube.com/watch?v=n0NAPG0ph5E'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.video_urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5164a44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for video in p.videos:\n",
    "    video.streams.first().download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d851c366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.youtube.com/watch?v=n0NAPG0ph5E\n",
      "https://www.youtube.com/watch?v=x2GX0akUvRw\n"
     ]
    }
   ],
   "source": [
    "for url in p.video_urls[:2]:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed555197",
   "metadata": {},
   "source": [
    "## Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a666101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.youtube.com/user/elbissan'\n",
    "c = Channel(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfc3fe7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bissan ElBaz'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.channel_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b79e480d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.youtube.com/user/elbissan/playlists'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.playlists_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "189ce0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.youtube.com/user/elbissan/videos'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.videos_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "895d4cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.youtube.com/watch?v=wmHiCB2H8Ig\n",
      "https://www.youtube.com/watch?v=keW2mpVzBNI\n",
      "https://www.youtube.com/watch?v=FVLOp_oNj_E\n"
     ]
    }
   ],
   "source": [
    "for url in c.video_urls[:3]:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b1bc23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2ae790bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "RegexMatchError",
     "evalue": "channel_name: could not find match for patterns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRegexMatchError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[1;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.youtube.com/feed/subscriptions\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m myChannels \u001b[38;5;241m=\u001b[39m \u001b[43mChannel\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytube\\contrib\\channel.py:24\u001b[0m, in \u001b[0;36mChannel.__init__\u001b[1;34m(self, url, proxies)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Construct a :class:`Channel <Channel>`.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m:param str url:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m    (Optional) A dictionary of proxies to use for web requests.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(url, proxies)\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel_uri \u001b[38;5;241m=\u001b[39m \u001b[43mextract\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchannel_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel_url \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.youtube.com\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel_uri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideos_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel_url \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/videos\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytube\\extract.py:185\u001b[0m, in \u001b[0;36mchannel_name\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m    182\u001b[0m         uri_identifier \u001b[38;5;241m=\u001b[39m function_match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muri_style\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muri_identifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 185\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m RegexMatchError(\n\u001b[0;32m    186\u001b[0m     caller\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, pattern\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatterns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    187\u001b[0m )\n",
      "\u001b[1;31mRegexMatchError\u001b[0m: channel_name: could not find match for patterns"
     ]
    }
   ],
   "source": [
    "url = 'https://www.youtube.com/feed/subscriptions'\n",
    "myChannels = Channel(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1c8ff48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.geeksforgeeks.org/introduction-to-web-scraping/'\n",
    "article = Article(url)\n",
    "article.download()\n",
    "article.parse()\n",
    "article.nlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "863e4878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article.authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5268822f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 11, 6, 12, 23, 4, tzinfo=tzutc())"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article.publish_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4c742f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user',\n",
       " 'sites',\n",
       " 'extraction',\n",
       " 'web',\n",
       " 'scraping',\n",
       " 'data',\n",
       " 'used',\n",
       " 'websites',\n",
       " 'information',\n",
       " 'website',\n",
       " 'introduction']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article.keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "18901549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Web scraping is a technique to fetch data from websites. While surfing on the web, many websites don’t allow the user to save data for personal use. One way is to manually copy-paste the data, which both tedious and time-consuming. Web Scraping is the automation of the data extraction process from websites. This event is done with the help of web scraping software known as web scrapers. They automatically load and extract data from the websites based on user requirements. These can be custom built to work for one site or can be configured to work with any website.\\n\\nUses of Web Scraping: Web scraping finds many uses both at a professional and personal level. Having different needs at different levels, some popular uses of web scraping are.\\n\\nBrand Monitoring and Competition Analysis: Web Scraping is used to get customer feedback regarding a particular service or product so as to understand how a customer feels regarding that particular thing. It is also used to extract competitor data in a structural, usable format.\\n\\nWeb Scraping is used to get customer feedback regarding a particular service or product so as to understand how a customer feels regarding that particular thing. It is also used to extract competitor data in a structural, usable format. Machine Learning: Machine Learning is a process of Artificial Intelligence in which the machine is allowed to learn and improve with its experience rather than being explicitly programmed. For that, a large amount of data is required from millions of sites which is extracted through web scraping software.\\n\\nMachine Learning is a process of Artificial Intelligence in which the machine is allowed to learn and improve with its experience rather than being explicitly programmed. For that, a large amount of data is required from millions of sites which is extracted through web scraping software. Financial Data Analysis: Web Scraping is used to keep a record of the stock market in a usable format and hence employ the same for insights.\\n\\nWeb Scraping is used to keep a record of the stock market in a usable format and hence employ the same for insights. Social Media Analysis: It is used to extract data from social media sites to gauge customer trends, and how they react to the campaign.\\n\\nIt is used to extract data from social media sites to gauge customer trends, and how they react to the campaign. SEO monitoring: Search Engine Optimization is the optimization of the visibility and ranking of a website among different search engines like Google, Yahoo, Bing, etc. Web scraping is used to understand how the ranking of the content over time. An there are so many other reasons to use Web Scrapping.\\n\\nTechniques of Web Scraping: There are two ways of extracting data from websites, the Manual extraction technique, and the automated extraction technique.\\n\\nManual Extraction Techniques: Manually copy-pasting the site content comes under this technique. Though tedious, time taking and repetitive it is an effective way to scrap data from the sites having good anti-scraping measures like bot detection.\\n\\nManually copy-pasting the site content comes under this technique. Though tedious, time taking and repetitive it is an effective way to scrap data from the sites having good anti-scraping measures like bot detection. Automated Extraction Techniques: Web scraping software is used to automatically extract data from sites based on user requirement. HTML Parsing: Parsing means to make something understandable to be analyzing it part by part. To wit, it means to convert the information in one form to another form that is easy to that is easier to work on with. HTML parsing means taking in the code and extracting relevant information from it based on the user requirement. Mainly executed using JavaScript, the target as the name suggests are HTML pages. DOM Parsing: The Document Object Model is the official recommendation of the World Wide Web Consortium. It defines an interface that enables a user to modify and update the style, structure, and content of the XML document. Web Scraping Software: Nowadays, many web scraping tools are available or are custom build on users need to extract required desiring information from millions of websites.\\n\\nWeb scraping software is used to automatically extract data from sites based on user requirement.\\n\\nTool for Web Scraping: Web Scraping tools are specifically developed for extracting data from the internet. Also, known as web harvesting tools or data extraction tools, they are useful for anyone trying to collect specific data from websites as they provide the user with structured data extracting data from a number of websites. Some of the most popular Web Scraping tools are:\\n\\nImport.io\\n\\nWebhose.io\\n\\nDexi.io\\n\\nScrapinghub\\n\\nParsehub\\n\\nLegalization of Web Scraping: The legalization of web scraping is a sensitive topic, depending on how it is used it can either be a boon or a bane. On one hand, web scraping with good bot enables search engines to index web content, price comparison services to save customer money and value. But web scraping can be re-targeted to meet more malicious and abusive ends. Web scraping can be aligned with other forms of malicious automation, named “bad bots”, which enable other harmful activities like denial of service attacks, competitive data mining, account hijacking, data theft etc.\\n\\nLegality of Web Scraping is a grey area that tends to develop as time goes on. Although the web scrapers technically increase the speed up data surfing, loading, copying, and pasting web scraping is also the key culprit behind the increases cases of copyright violation, violated terms of use and other activities that are highly disruptive to a company’s business.\\n\\nChallenges to Web Scraping: Besides the challenge of the legality of web scraping, there are also other problems that pose a challenge to web scraping.\\n\\nData Warehousing: Data extraction at a scale will generate a large amount of information to be stored. If the data warehousing infrastructure is not properly built then the searching, storing and exporting of this data will become a cumbersome task. Hence, for large-scale data extraction, there needs to be a perfect data warehousing system without any flaws and faults.\\n\\nData extraction at a scale will generate a large amount of information to be stored. If the data warehousing infrastructure is not properly built then the searching, storing and exporting of this data will become a cumbersome task. Hence, for large-scale data extraction, there needs to be a perfect data warehousing system without any flaws and faults. Website Structure Changes: Every website periodically updates its user interface to improve its attractiveness and experience. This requires various structural changes too. Since the web scrapers are set up according to the code elements of the website at that time, they require changes too. So, they require changes weekly too to target the correct website for data scraping as incomplete information regarding the website structure will lead to improper scraping of data.\\n\\nEvery website periodically updates its user interface to improve its attractiveness and experience. This requires various structural changes too. Since the web scrapers are set up according to the code elements of the website at that time, they require changes too. So, they require changes weekly too to target the correct website for data scraping as incomplete information regarding the website structure will lead to improper scraping of data. Anti-Scraping Technologies: Some websites use anti-scraping technologies that thwart away any scraping attempt. They apply a dynamic coding algorithm to prevent any bot intervention and use the IP blocking mechanism. It requires a lot of time and money to work around such anti-scraping technologies.\\n\\nSome websites use anti-scraping technologies that thwart away any scraping attempt. They apply a dynamic coding algorithm to prevent any bot intervention and use the IP blocking mechanism. It requires a lot of time and money to work around such anti-scraping technologies. Quality of Data Extracted: Records that do not meet the quality of information required will affect the overall integrity of the data. Making sure that the Data Scraped meets the quality guidelines is a difficult task as it needs to be done in real-time.\\n\\nFuture of Data Scraping: As there are some challenges and opportunities for data scraping, it can be fairly deemed that the unintended data-scraping practitioners are prone to create a moral hazard where they target the companies and retrieve their data. However, since we are on the verge of data transformation, data-scraping in combination with big data can provide the company’s market intelligence and help them identify critical trends and patterns and identify the best opportunities and solutions. Hence, it won’t be wrong to say that Data scraping can be upgraded to the better soon.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8894c7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
